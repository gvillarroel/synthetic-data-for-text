{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install optuna pytorch_lightning rouge-score transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545870, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#.replace(to_replace=\"-1\", value=np.nan)\n",
    "df = pd.read_parquet('../datasets/economicos/synth/split/train.parquet').replace(to_replace=\"None\", value=np.nan).replace(to_replace=-1, value=np.nan)\n",
    "display(df.shape)\n",
    "df.sample(3)\n",
    "CHAR_SEP = \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Describe', '2019-12-05 precio $ 60.000 tipo Casa Amoblada transacción Arriendo región Valparaíso comuna Algarrobo dormitorios 3.0 baños 3.0 constuidos 140.0 terreno 300.0 precio_real 2.1231550166483135:'], 'target': 'Se arrienda linda y cómoda casa de veraneo. Completamente equipada hasta para 6 personas. Cuenta con jardín, quincho, mesa de Pool, etc.'}\n"
     ]
    }
   ],
   "source": [
    "def convert(row):\n",
    "    return {\n",
    "        \"text\": [\"Describe\", f\"\"\"{row.publication_date.strftime('%Y-%m-%d')}\n",
    "precio {row.price}\n",
    "tipo {row.property_type}\n",
    "transacción {row.transaction_type}\n",
    "región {row.state}\n",
    "comuna {row.county}\n",
    "dormitorios {row.rooms}\n",
    "baños {row.rooms}\n",
    "constuidos {row.m_built}\n",
    "terreno {row.m_size}\n",
    "precio_real {row._price}:\"\"\".replace(\"\\n\", \" \")],\n",
    "        \"target\": row.description\n",
    "        }\n",
    "\n",
    "print(\n",
    "    df.sample(1).apply(convert, axis=1).iloc[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Describe', '2022-02-23 precio 10.700 UF tipo Departamento transacción Venta región Metropolitana de Santiago comuna Vitacura dormitorios 3.0 baños 3.0 constuidos 114.0 terreno 131.0 precio_real 10700.0:'], 'target': 'Departamento en calle tranquila a pasos de Costanera Norte, bonito sector residencial caminable al Parque Bicentenario, Comercio y Restaurantes. Hall de entrada, recibos juntos con salida a agradable terraza; Cocina con amplia logia, muebles con cubierta de cuarzo; Sector dormitorios con puerta que lo independiza del hall. Suite principal amplia con W-closet , mas otro closet al muro, baño; otros 2 dormitorios que comparten baño.Todos los dormitorios con salida a la terraza. Recibos, hall y dormitorios con piso laminado; Calefacción. Son 114 m2 útiles mas 22m2 ( entre terraza 17 m y logia 5m) Edificio con muy buenas terminaciones 2 estacionamientos, bodega. Jardín y piscina. Seguridad las 24 hrs. Estacionamientos para visitas. Caminable a Colegio Ursulinas; buenas vias de acceso. Vale la pena visitar.'}\n"
     ]
    }
   ],
   "source": [
    "def convert2(row):\n",
    "    return {\n",
    "        \"text\": [\"Descripción:\"] + list(map(str, [\n",
    "            row.publication_date.strftime('%Y-%m-%d'),\n",
    "            row.price,\n",
    "            row.property_type,\n",
    "            row.transaction_type,\n",
    "            row.state,\n",
    "            row.county,\n",
    "            row.rooms,\n",
    "            row.bathrooms,\n",
    "            row.m_built,\n",
    "            row.m_size,\n",
    "            row._price\n",
    "        ])),\n",
    "        \"target\": CHAR_SEP.join(row.description.split())\n",
    "        }\n",
    "\n",
    "print(\n",
    "    df.sample(1).apply(convert, axis=1).iloc[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>[Describe, 2018-06-27 precio $ tipo nan transa...</td>\n",
       "      <td>se vende una hectaria terreno indigena Maquehu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35613</th>\n",
       "      <td>[Describe, 2019-01-08 precio 33 UF tipo Oficin...</td>\n",
       "      <td>Se arrienda oficina centro de viña del mar ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234412</th>\n",
       "      <td>[Describe, 2022-01-26 precio $ 415.000 tipo De...</td>\n",
       "      <td>Arriendo departamento dos dormitorios y un ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "7983    [Describe, 2018-06-27 precio $ tipo nan transa...   \n",
       "35613   [Describe, 2019-01-08 precio 33 UF tipo Oficin...   \n",
       "234412  [Describe, 2022-01-26 precio $ 415.000 tipo De...   \n",
       "\n",
       "                                                   target  \n",
       "7983    se vende una hectaria terreno indigena Maquehu...  \n",
       "35613   Se arrienda oficina centro de viña del mar ed...  \n",
       "234412  Arriendo departamento dos dormitorios y un ban...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.DataFrame(df.apply(convert, axis=1).to_list())\n",
    "df_text.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rouge_score import rouge_scorer\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import  DataLoader, RandomSampler, SequentialSampler #Dataset,\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = T5Tokenizer.from_pretrained(\"google/t5-v1_1-xxl\")\n",
    "def t2l(row):\n",
    "    text = row.text\n",
    "    target = row.target\n",
    "\n",
    "    return {\"text\": len(tkn(\n",
    "        text\n",
    "        )[\"input_ids\"]),\n",
    "        \"target\": len(tkn(\n",
    "        target\n",
    "        )[\"input_ids\"])}\n",
    "#txttgt = pd.DataFrame(df.apply(convert, axis=1).to_list())\n",
    "#d = pd.DataFrame(txttgt).apply(t2l, axis=1).to_list()\n",
    "#print(pd.DataFrame(d).quantile(0.1))\n",
    "#print(pd.DataFrame(d).quantile(0.96))\n",
    "## in: 12:36\n",
    "## out: 12:528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SEP = \" \"\n",
    "MAX_SRC_LEN = 150\n",
    "MAX_TGT_LEN = 720\n",
    "class T5Finetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, df, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.args = args\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.args.model)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.args.model)\n",
    "        self.data = df\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _encode_text(self, text_input, target):\n",
    "      ctext = str(text_input)\n",
    "      ctext = CHAR_SEP.join(ctext.split())\n",
    "      target = str(target) #summarized text\n",
    "      target = CHAR_SEP.join(target.split())\n",
    "      source = self.tokenizer.batch_encode_plus([ctext], \n",
    "                                                max_length= MAX_SRC_LEN, \n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "      target = self.tokenizer.batch_encode_plus([target], \n",
    "                                                max_length=MAX_TGT_LEN,\n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "      y = target['input_ids']\n",
    "      target_id = y[:, :-1].contiguous()\n",
    "      target_label = y[:, 1:].clone().detach()\n",
    "      target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
    "      return source['input_ids'], source['attention_mask'], target_id, target_label\n",
    "    \n",
    "    def encode_text(self, text, target):\n",
    "        source = self.tokenizer.batch_encode_plus([text], \n",
    "                                                max_length= MAX_SRC_LEN, \n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target], \n",
    "                                                max_length=MAX_TGT_LEN,\n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "        y = target['input_ids']\n",
    "        target_id = y[:, :-1].contiguous()\n",
    "        target_label = y[:, 1:].clone().detach()\n",
    "        target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
    "        return source['input_ids'], source['attention_mask'], target_id, target_label\n",
    "\n",
    "        \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        source_ids, source_masks, target_ids, target_labels = [], [], [], [] \n",
    "        for _, row in self.data.iterrows():\n",
    "            source_id, source_mask, target_id, target_label = self.encode_text(row.text, row.target)\n",
    "            source_ids.append(source_id)\n",
    "            source_masks.append(source_mask)\n",
    "            target_ids.append(target_id)\n",
    "            target_labels.append(target_label)\n",
    "\n",
    "        # Convert the lists into tensors\n",
    "        source_ids = torch.cat(source_ids, dim=0)\n",
    "        source_masks = torch.cat(source_masks, dim=0)\n",
    "        target_ids = torch.cat(target_ids, dim=0)\n",
    "        target_labels = torch.cat(target_labels, dim=0)\n",
    "        # splitting the data to train, validation, and test\n",
    "        data = TensorDataset(source_ids, source_masks, target_ids, target_labels)\n",
    "        train_size, val_size = int(0.8 * len(data)), int(0.1 * len(data))\n",
    "        test_size = len(data) - (train_size + val_size)\n",
    "        self.train_dat, self.val_dat, self.test_dat = \\\n",
    "            random_split(data, [train_size, val_size, test_size])\n",
    "    \n",
    "    def forward(self, batch, batch_idx):\n",
    "        source_ids, source_mask, target_ids, target_labels = batch[:4]\n",
    "        return self.model(input_ids = source_ids, attention_mask = source_mask, \n",
    "                          decoder_input_ids=target_ids, labels=target_labels)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss, 'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
    "        out = {'val_loss': loss}\n",
    "        return {**out, 'log': out}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
    "        out = {'test_loss': loss}\n",
    "        return {**out, 'log': out}\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dat, batch_size=self.batch_size,\n",
    "                          num_workers=4, sampler=RandomSampler(self.train_dat))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dat, batch_size=self.args.bs, num_workers=4,\n",
    "                          sampler=SequentialSampler(self.val_dat))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dat, batch_size=self.args.bs, num_workers=4,\n",
    "                          sampler=SequentialSampler(self.test_dat))    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.args.lr, eps=1e-4)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0,\n",
    "            num_training_steps=self.args.max_epochs * len(self.train_dat))\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "    \n",
    "    def generate_summary(self, ctext, summ_len=150, text='', beam_search=2, repetition_penalty=2.5):\n",
    "        source_id, source_mask, target_id, target_label = self.encode_text(ctext, text)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids = source_id,\n",
    "                attention_mask = source_mask, \n",
    "                max_length=summ_len, \n",
    "                truncation=True,\n",
    "                num_beams=beam_search,\n",
    "                repetition_penalty=repetition_penalty, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            prediction = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        if len(text) > 0:\n",
    "            target = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in target_id]\n",
    "            scores = self.scorer.score(target[0], prediction[0])\n",
    "            return prediction, scores\n",
    "        else:\n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    def save_core_model(self):\n",
    "        store_path = join(self.args.output, self.args.name, 'core')\n",
    "        self.model.save_pretrained(store_path)\n",
    "        self.tokenizer.save_pretrained(store_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        p = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        p.add_argument('-m', '--model', type=str, default='t5-base',\n",
    "                       help='name of the model or the path pointing to it')\n",
    "        p.add_argument('--bs', '--batch_size', type=int, default=2)\n",
    "        p.add_argument('--source_len', type=int, default=120)\n",
    "        p.add_argument('--summ_len', type=int, default=700)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_args():\n",
    "    p = ArgumentParser()\n",
    "    p.add_argument('-p', '--path', type=str,  \n",
    "                   default='/content/gdrive/My Drive/Colab Notebooks/data/text_summarization_t5/news_summary.csv',\n",
    "                  help='path to the data file')\n",
    "    p.add_argument('-o', '--output', type=str, default='/tmp/tpu-template',\n",
    "                  help='path to the output directory for storing the model')\n",
    "    p.add_argument('-n', '--name', type=str, default='google/t5-v1_1-xxl',\n",
    "                  help='this name will be used on tensorboard for the model')\n",
    "    p.add_argument('-t', '--trials', type=int, default=1,\n",
    "                  help='number of trials for hyperparameter search')\n",
    "    p.add_argument('--seed', type=int, default=0, help='randomization seed')\n",
    "    p = T5Finetuner.add_model_specific_args(p)\n",
    "    p = pl.Trainer.add_argparse_args(p)\n",
    "    args,_ = p.parse_known_args()\n",
    "    args.max_epochs = 2\n",
    "    return args\n",
    "\n",
    "def default_args():\n",
    "    p = ArgumentParser()\n",
    "    args,_ = p.parse_known_args()\n",
    "    args.max_epochs = 2\n",
    "    args.model = \"google/flan-t5-small\"\n",
    "    args.output = f\"./{args.model.replace('/','_')}\"\n",
    "    args.name = \"DESCRIPCION_PROPIEDADES\"\n",
    "    args.bs = 10 # batch size\n",
    "    return args\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    args = default_args()\n",
    "    # sampling the hyperparameters\n",
    "    args.lr = trial.suggest_categorical(\"lr\", [1e-6, 5e-6, 1e-5, 5e-5, 1e-4])\n",
    "    # setting up the right callbacks\n",
    "    cp_callback = pl.callbacks.ModelCheckpoint(\n",
    "        join(args.output, args.name, f\"trial_{trial.number}\", \"{epoch}\"),\n",
    "        monitor=\"val_loss\", mode=\"min\")\n",
    "    pr_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    metrics_callback = MetricsCallback()\n",
    "    summarizer = T5Finetuner(args, df_text)         # loading the model\n",
    "    trainer = pl.Trainer.from_argparse_args(      # loading the trainer\n",
    "        args, \n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        default_root_dir=args.output, gradient_clip_val=1.0,\n",
    "        #checkpoint_callback=cp_callback,\n",
    "        callbacks=[metrics_callback, cp_callback, pr_callback],\n",
    "        #early_stop_callback=pr_callback, \n",
    "        num_sanity_val_steps=-1,\n",
    "        auto_scale_batch_size=\"power\",\n",
    "        # select TensorBoad or Wandb logger\n",
    "        logger=TensorBoardLogger(join(args.output, 'logs'), name=args.name, version=f'trial_{trial.number}')\n",
    "        )\n",
    "  \n",
    "    trainer.fit(summarizer)                       # fitting the model\n",
    "    trainer.test(summarizer)                      # testing the model\n",
    "    return min([x['val_loss'].item() for x in metrics_callback.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-26 15:37:57,477]\u001b[0m A new study created in memory with name: no-name-d4d0ed9b-f253-48fe-84a0-d0683c6fa44f\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 77.0 M\n",
      "-----------------------------------------------------\n",
      "77.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "77.0 M    Total params\n",
      "307.845   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 49129/49129 [1:50:49<00:00,  7.39it/s, loss=2.17, v_num=al_0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 49129/49129 [1:50:53<00:00,  7.38it/s, loss=2.17, v_num=al_0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 5459/5459 [03:59<00:00, 22.75it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-12-26 19:46:25,972]\u001b[0m Trial 0 failed because of the following error: KeyError('val_loss')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_43454/1754415690.py\", line 55, in optuna_objective\n",
      "    return min([x['val_loss'].item() for x in metrics_callback.metrics])\n",
      "  File \"/tmp/ipykernel_43454/1754415690.py\", line 55, in <listcomp>\n",
      "    return min([x['val_loss'].item() for x in metrics_callback.metrics])\n",
      "KeyError: 'val_loss'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study()\n\u001b[0;32m----> 2\u001b[0m study\u001b[39m.\u001b[39;49moptimize(optuna_objective, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [11], line 55\u001b[0m, in \u001b[0;36moptuna_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     53\u001b[0m trainer\u001b[39m.\u001b[39mfit(summarizer)                       \u001b[39m# fitting the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m trainer\u001b[39m.\u001b[39mtest(summarizer)                      \u001b[39m# testing the model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m([x[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m metrics_callback\u001b[39m.\u001b[39mmetrics])\n",
      "Cell \u001b[0;32mIn [11], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m trainer\u001b[39m.\u001b[39mfit(summarizer)                       \u001b[39m# fitting the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m trainer\u001b[39m.\u001b[39mtest(summarizer)                      \u001b[39m# testing the model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m([x[\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m metrics_callback\u001b[39m.\u001b[39mmetrics])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(optuna_objective, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL=\"/home/gvillarroel/dev/synthetic-data-for-text/notebooks/google_flan-t5-small/logs/DESCRIPCION_PROPIEDADES/trial_0/checkpoints/epoch=1-step=87340.ckpt\"\n",
    "new_model = T5Finetuner.load_from_checkpoint(NEW_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = new_model.tokenizer.batch_encode_plus([df_text.iloc[200].text], \n",
    "                                                max_length= MAX_SRC_LEN, \n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    3, 30010,     1,   846, 18083, 10106,     3,   102,  7886,    32,\n",
       "          1514,  2226,    32,  9882,   736,    32,  4605,    26,     9,  3017,\n",
       "             9,    75, 12765,  1533,  3483,    26,    32,  5925,    23, 15742,\n",
       "          3144, 29461,  6252,     9,     3,    29,   152, 28349,    23,    32,\n",
       "             7,     3,    29,   152,  4698,     2,    32,     7,     3,    29,\n",
       "           152,  6900,    17,    76, 28594,     3,    29,   152, 10225,    29,\n",
       "            32,     3,    29,   152,     3,   102,  7886,    32,   834,  6644,\n",
       "             3, 11739,    10,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.eval()\n",
    "y_hat = new_model.model.generate(\n",
    "    inputs=encoded['input_ids'],\n",
    "    attention_mask=encoded['attention_mask'],\n",
    "    num_beams=1,\n",
    "    max_length=500,\n",
    "    repetition_penalty=2.5,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True,\n",
    "    use_cache=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arriendo casa amoblada, 2 dormitoriOS. 998283509.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([new_model.tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for gen_id in y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_model\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mdecode(y_hat\u001b[39m.\u001b[39;49mlogits\u001b[39m.\u001b[39;49msqueeze(), skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3468\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3466\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3469\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3470\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3471\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3472\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3473\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    928\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    933\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "new_model.tokenizer.decode(y_hat.logits.squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d  = new_model.encode_text(\n",
    "        ['Describe',\n",
    " '2018-04-15 precio $ tipo Casa Amoblada transacción Arriendo región Los Lagos comuna nan dormitorios nan baños nan constuidos nan terreno nan precio_real 0.0:'], \"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "339c1ec44efa671d0c0aa472f92368e3220a4c3534888d9daac06fdbcd7c45f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
