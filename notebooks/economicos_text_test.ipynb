{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WigXlQ457KM7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U optuna pytorch_lightning==1.8.5.post0 rouge-score transformers sentencepiece pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NVb5ZoMf7KM9",
        "outputId": "db5a11c7-49b0-4bea-ca6a-30fa8b7b1b32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(545870, 17)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>description</th>\n",
              "      <th>price</th>\n",
              "      <th>property_type</th>\n",
              "      <th>transaction_type</th>\n",
              "      <th>state</th>\n",
              "      <th>county</th>\n",
              "      <th>publication_date</th>\n",
              "      <th>rooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>m_built</th>\n",
              "      <th>m_size</th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>address</th>\n",
              "      <th>owner</th>\n",
              "      <th>_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>193185</th>\n",
              "      <td>https://www.economicos.cl/propiedades/vendo-de...</td>\n",
              "      <td>Vendo depto. nuevo, piso 9 y último piso con ...</td>\n",
              "      <td>9500 UF</td>\n",
              "      <td>Departamento</td>\n",
              "      <td>Venta</td>\n",
              "      <td>Metropolitana de Santiago</td>\n",
              "      <td>Providencia</td>\n",
              "      <td>2019-03-27 21:44:03</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8304.0</td>\n",
              "      <td>9586.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Vendo depto. nuevo 3 dormitorios 9.500 UF</td>\n",
              "      <td>Silvina Hurtado / Antonio Varas Providencia, M...</td>\n",
              "      <td>-1</td>\n",
              "      <td>9500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291391</th>\n",
              "      <td>https://www.economicos.cl/propiedades/departam...</td>\n",
              "      <td>420.000 3 dormitorios acceso controlado  99435...</td>\n",
              "      <td>$ 420.000</td>\n",
              "      <td>Departamento</td>\n",
              "      <td>Arriendo</td>\n",
              "      <td>Metropolitana de Santiago</td>\n",
              "      <td>Maipú</td>\n",
              "      <td>2018-10-31 00:00:27</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>El Mercurio</td>\n",
              "      <td>Departamento en Arriendo en Maipú 3 dormitorios</td>\n",
              "      <td>Maipú, Metropolitana de Santiago</td>\n",
              "      <td>-1</td>\n",
              "      <td>15.310531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743799</th>\n",
              "      <td>https://www.economicos.cl/propiedades/departam...</td>\n",
              "      <td>Se arrienda hermoso y amplio departamento en M...</td>\n",
              "      <td>$ 380.000</td>\n",
              "      <td>Departamento</td>\n",
              "      <td>Arriendo</td>\n",
              "      <td>Metropolitana de Santiago</td>\n",
              "      <td>Macul</td>\n",
              "      <td>2020-11-19 12:25:08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Departamento a pasos del metro Las Torres</td>\n",
              "      <td>Macul, Metropolitana de Santiago</td>\n",
              "      <td>Katherine Torres</td>\n",
              "      <td>13.123355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      url  \\\n",
              "193185  https://www.economicos.cl/propiedades/vendo-de...   \n",
              "291391  https://www.economicos.cl/propiedades/departam...   \n",
              "743799  https://www.economicos.cl/propiedades/departam...   \n",
              "\n",
              "                                              description      price  \\\n",
              "193185  Vendo depto. nuevo, piso 9 y último piso con ...    9500 UF   \n",
              "291391  420.000 3 dormitorios acceso controlado  99435...  $ 420.000   \n",
              "743799  Se arrienda hermoso y amplio departamento en M...  $ 380.000   \n",
              "\n",
              "       property_type transaction_type                      state       county  \\\n",
              "193185  Departamento            Venta  Metropolitana de Santiago  Providencia   \n",
              "291391  Departamento         Arriendo  Metropolitana de Santiago        Maipú   \n",
              "743799  Departamento         Arriendo  Metropolitana de Santiago        Macul   \n",
              "\n",
              "          publication_date  rooms  bathrooms  m_built  m_size       source  \\\n",
              "193185 2019-03-27 21:44:03    3.0        2.0   8304.0  9586.0          NaN   \n",
              "291391 2018-10-31 00:00:27    3.0        NaN      NaN     NaN  El Mercurio   \n",
              "743799 2020-11-19 12:25:08    2.0        2.0      NaN     NaN          NaN   \n",
              "\n",
              "                                                  title  \\\n",
              "193185        Vendo depto. nuevo 3 dormitorios 9.500 UF   \n",
              "291391  Departamento en Arriendo en Maipú 3 dormitorios   \n",
              "743799        Departamento a pasos del metro Las Torres   \n",
              "\n",
              "                                                  address              owner  \\\n",
              "193185  Silvina Hurtado / Antonio Varas Providencia, M...                 -1   \n",
              "291391                   Maipú, Metropolitana de Santiago                 -1   \n",
              "743799                   Macul, Metropolitana de Santiago   Katherine Torres   \n",
              "\n",
              "             _price  \n",
              "193185  9500.000000  \n",
              "291391    15.310531  \n",
              "743799    13.123355  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#.replace(to_replace=\"-1\", value=np.nan)\n",
        "df = pd.read_parquet('../datasets/economicos/synth/split/train.parquet').replace(to_replace=\"None\", value=np.nan).replace(to_replace=-1, value=np.nan)\n",
        "display(df.shape)\n",
        "\n",
        "CHAR_SEP = \" \"\n",
        "df.sample(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ['Describe', '2018-06-15 precio $ 366.000.000 tipo Casa transacción Venta región Metropolitana de Santiago comuna Providencia dormitorios 5.0 baños 5.0 constuidos 210.0 terreno 275.0 precio_real 13496.511247776923'], 'target': 'Amplia Casa, cerca de la Estación Metro Baquedano. \\nHERMOSA CASA SEÑORIAL.\\nHall de entrada, amplio living separado de amplio comedor, 4 dormitorios, 2 baños, cocina amplia, pieza y baño de empleada, patio, garage para un auto.\\nRevestimiento piso: madera.\\nMetros útiles: 210 m2.\\nMetros terreno: 275 m2.\\nCerca de supermercados, bancos, comercios, colegios, etc.\\nValor: $ 366.000.000.-\\n'}\n"
          ]
        }
      ],
      "source": [
        "def convert(row):\n",
        "    return {\n",
        "        \"text\": [\"Describe\", f\"\"\"{row.publication_date.strftime('%Y-%m-%d')}\n",
        "precio {row.price}\n",
        "tipo {row.property_type}\n",
        "transacción {row.transaction_type}\n",
        "región {row.state}\n",
        "comuna {row.county}\n",
        "dormitorios {row.rooms}\n",
        "baños {row.rooms}\n",
        "constuidos {row.m_built}\n",
        "terreno {row.m_size}\n",
        "precio_real {row._price}\"\"\".replace(\"\\n\", \" \")],\n",
        "        \"target\": row.description\n",
        "        }\n",
        "\n",
        "print(\n",
        "    df.sample(1).apply(convert, axis=1).iloc[-1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "MxDRMWrT7KM_",
        "outputId": "b91151da-aeb9-49c4-9a13-b20adbc61112"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8851</th>\n",
              "      <td>[Describe, 2022-02-23 precio $ 299.040.000 tip...</td>\n",
              "      <td>Se vende casa en condominio excelente estado, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328228</th>\n",
              "      <td>[Describe, 2019-03-19 precio $ 58.000.000 tipo...</td>\n",
              "      <td>Arriendo departamento en Quilpué, sector Paso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212342</th>\n",
              "      <td>[Describe, 2018-02-23 precio 6300 UF tipo Depa...</td>\n",
              "      <td>Se vende departamento hermosa vista al mar. 3 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  \\\n",
              "8851    [Describe, 2022-02-23 precio $ 299.040.000 tip...   \n",
              "328228  [Describe, 2019-03-19 precio $ 58.000.000 tipo...   \n",
              "212342  [Describe, 2018-02-23 precio 6300 UF tipo Depa...   \n",
              "\n",
              "                                                   target  \n",
              "8851    Se vende casa en condominio excelente estado, ...  \n",
              "328228  Arriendo departamento en Quilpué, sector Paso...  \n",
              "212342  Se vende departamento hermosa vista al mar. 3 ...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_text = pd.DataFrame(df.apply(convert, axis=1).to_list())\n",
        "df_text.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YeWDE1mI7KNA"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "from os.path import join, isfile\n",
        "from os import listdir\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from rouge_score import rouge_scorer\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import  DataLoader, RandomSampler, SequentialSampler #Dataset,\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "38Km3oVD7KNB"
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(pl.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_BEznZWG7KNB"
      },
      "outputs": [],
      "source": [
        "CHAR_SEP = \" \"\n",
        "MAX_SRC_LEN = 150\n",
        "MAX_TGT_LEN = 720\n",
        "from functools import partial\n",
        "\n",
        "class T5Finetuner(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, args, df, batch_size=8):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.args = args\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(self.args.model)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(self.args.model)\n",
        "        self.data = df\n",
        "        #self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.decode = partial(self.tokenizer.decode, skip_special_tokens=True)\n",
        "\n",
        "    def _encode_text(self, text_input, target):\n",
        "      ctext = str(text_input)\n",
        "      ctext = CHAR_SEP.join(ctext.split())\n",
        "      target = str(target) #summarized text\n",
        "      target = CHAR_SEP.join(target.split())\n",
        "      source = self.tokenizer.batch_encode_plus([ctext], \n",
        "                                                max_length= MAX_SRC_LEN, \n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "      target = self.tokenizer.batch_encode_plus([target], \n",
        "                                                max_length=MAX_TGT_LEN,\n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "      y = target['input_ids']\n",
        "      target_id = y[:, :-1].contiguous()\n",
        "      target_label = y[:, 1:].clone().detach()\n",
        "      target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
        "      return source['input_ids'], source['attention_mask'], target_id, target_label\n",
        "    \n",
        "    def encode_text(self, text, target):\n",
        "        source = self.tokenizer.batch_encode_plus([text], \n",
        "                                                max_length= MAX_SRC_LEN, \n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([target], \n",
        "                                                max_length=MAX_TGT_LEN,\n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "        y = target['input_ids']\n",
        "        target_id = y[:, :-1].contiguous()\n",
        "        target_label = y[:, 1:].clone().detach()\n",
        "        target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
        "        return source['input_ids'], source['attention_mask'], target_id, target_label\n",
        "\n",
        "    \n",
        "    def prepare_df(self, df):\n",
        "        source_ids, source_masks, target_ids, target_labels = [], [], [], [] \n",
        "        for _, row in df.iterrows():\n",
        "            source_id, source_mask, target_id, target_label = self.encode_text(row.text, row.target)\n",
        "            source_ids.append(source_id)\n",
        "            source_masks.append(source_mask)\n",
        "            target_ids.append(target_id)\n",
        "            target_labels.append(target_label)\n",
        "\n",
        "        # Convert the lists into tensors\n",
        "        source_ids = torch.cat(source_ids, dim=0)\n",
        "        source_masks = torch.cat(source_masks, dim=0)\n",
        "        target_ids = torch.cat(target_ids, dim=0)\n",
        "        target_labels = torch.cat(target_labels, dim=0)\n",
        "        # splitting the data to train, validation, and test\n",
        "        return TensorDataset(source_ids, source_masks, target_ids, target_labels)\n",
        "    \n",
        "    def prepare_data(self):\n",
        "        # splitting the data to train, validation, and test\n",
        "        data = self.prepare_df(self.data)\n",
        "        train_size, val_size = int(0.8 * len(data)), int(0.1 * len(data))\n",
        "        test_size = len(data) - (train_size + val_size)\n",
        "        self.train_dat, self.val_dat, self.test_dat = \\\n",
        "            random_split(data, [train_size, val_size, test_size])\n",
        "    \n",
        "    def forward(self, batch, batch_idx):\n",
        "        source_ids, source_mask, target_ids, target_labels = batch[:4]\n",
        "        return self.model(input_ids = source_ids, attention_mask = source_mask, \n",
        "                          decoder_input_ids=target_ids, labels=target_labels)\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self(batch, batch_idx)[0]\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
        "        return self(batch, batch_idx)\n",
        "        return \" \".join(map(self.decode, y_hat))\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self(batch, batch_idx)[0]\n",
        "        return {'loss': loss, 'val_loss': loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
        "        out = {'val_loss': loss}\n",
        "        return {**out, 'log': out}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self(batch, batch_idx)[0]\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
        "        out = {'test_loss': loss}\n",
        "        return {**out, 'log': out}\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dat, batch_size=self.batch_size,\n",
        "                          num_workers=4, sampler=RandomSampler(self.train_dat))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dat, batch_size=self.args.bs, num_workers=4,\n",
        "                          sampler=SequentialSampler(self.val_dat))\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dat, batch_size=self.args.bs, num_workers=4,\n",
        "                          sampler=SequentialSampler(self.test_dat))    \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.args.lr, eps=1e-4)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0,\n",
        "            num_training_steps=self.args.max_epochs * len(self.train_dat))\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "    \n",
        "    def generate_summary(self, ctext, summ_len=150, text='', beam_search=2, repetition_penalty=2.5):\n",
        "        source_id, source_mask, target_id, target_label = self.encode_text(ctext, text)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated_ids = self.model.generate(\n",
        "                input_ids = source_id,\n",
        "                attention_mask = source_mask, \n",
        "                max_length=summ_len, \n",
        "                truncation=True,\n",
        "                num_beams=beam_search,\n",
        "                repetition_penalty=repetition_penalty, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            prediction = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "        if len(text) > 0:\n",
        "            target = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in target_id]\n",
        "            scores = self.scorer.score(target[0], prediction[0])\n",
        "            return prediction, scores\n",
        "        else:\n",
        "            return prediction\n",
        "        \n",
        "\n",
        "    def save_core_model(self):\n",
        "        store_path = join(self.args.output, self.args.name, 'core')\n",
        "        self.model.save_pretrained(store_path)\n",
        "        self.tokenizer.save_pretrained(store_path)\n",
        "        \n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        p = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        p.add_argument('-m', '--model', type=str, default='t5-base',\n",
        "                       help='name of the model or the path pointing to it')\n",
        "        p.add_argument('--bs', '--batch_size', type=int, default=2)\n",
        "        p.add_argument('--source_len', type=int, default=120)\n",
        "        p.add_argument('--summ_len', type=int, default=700)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "INWGt3Rd7KNC"
      },
      "outputs": [],
      "source": [
        "def default_args():\n",
        "    p = ArgumentParser()\n",
        "    p.add_argument('-p', '--path', type=str,  \n",
        "                   default='/content/gdrive/My Drive/Colab Notebooks/data/text_summarization_t5/news_summary.csv',\n",
        "                  help='path to the data file')\n",
        "    p.add_argument('-o', '--output', type=str, default='/tmp/tpu-template',\n",
        "                  help='path to the output directory for storing the model')\n",
        "    p.add_argument('-n', '--name', type=str, default='google/t5-v1_1-xxl',\n",
        "                  help='this name will be used on tensorboard for the model')\n",
        "    p.add_argument('-t', '--trials', type=int, default=1,\n",
        "                  help='number of trials for hyperparameter search')\n",
        "    p.add_argument('--seed', type=int, default=0, help='randomization seed')\n",
        "    p = T5Finetuner.add_model_specific_args(p)\n",
        "    p = pl.Trainer.add_argparse_args(p)\n",
        "    args,_ = p.parse_known_args()\n",
        "    args.max_epochs = 2\n",
        "    return args\n",
        "\n",
        "def default_args():\n",
        "    p = ArgumentParser()\n",
        "    args,_ = p.parse_known_args()\n",
        "    args.max_epochs = 2\n",
        "    #args.model = \"google/flan-t5-small\"\n",
        "    #args.model = \"google/flan-t5-xl\"\n",
        "    args.model = \"google/flan-t5-large\"\n",
        "    #args.model = \"google/flan-t5-base\"\n",
        "    args.output = f\"./{args.model.replace('/','_')}\"\n",
        "    args.name = \"DESCRIPCION_PROPIEDADES\"\n",
        "    args.bs = 1 # batch size\n",
        "    return args\n",
        "\n",
        "def optuna_objective(trial):\n",
        "    args = default_args()\n",
        "    # sampling the hyperparameters\n",
        "    args.lr = trial.suggest_categorical(\"lr\", [1e-6, 5e-6, 1e-5, 5e-5, 1e-4])\n",
        "    # setting up the right callbacks\n",
        "    cp_callback = pl.callbacks.ModelCheckpoint(\n",
        "        join(args.output, args.name, f\"trial_{trial.number}\", \"{epoch}\"),\n",
        "        monitor=\"val_loss\", mode=\"min\")\n",
        "    pr_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
        "    metrics_callback = MetricsCallback()\n",
        "    summarizer = T5Finetuner(args, df_text)         # loading the model\n",
        "    trainer = pl.Trainer.from_argparse_args(      # loading the trainer\n",
        "        args, \n",
        "        accelerator=\"gpu\",\n",
        "        devices=1,\n",
        "        default_root_dir=args.output, gradient_clip_val=1.0,\n",
        "        #checkpoint_callback=cp_callback,\n",
        "        callbacks=[metrics_callback],\n",
        "        #early_stop_callback=pr_callback, \n",
        "        num_sanity_val_steps=-1,\n",
        "        #auto_scale_batch_size=\"power\",\n",
        "        # select TensorBoad or Wandb logger\n",
        "        logger=TensorBoardLogger(join(args.output, 'logs'), name=args.name, version=f'trial_{trial.number}')\n",
        "        )\n",
        "  \n",
        "    trainer.fit(summarizer)                       # fitting the model\n",
        "    trainer.test(summarizer)                      # testing the model\n",
        "    return min([x['val_loss'].item() for x in metrics_callback.metrics])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8Tv_jey87KND"
      },
      "outputs": [],
      "source": [
        "#NEW_MODEL=\"/home/gvillarroel/dev/synthetic-data-for-text/notebooks/google_flan-t5-base/logs/DESCRIPCION_PROPIEDADES/trial_0/checkpoints/epoch=1-step=436696.ckpt\"\n",
        "NEW_MODEL = \"/home/gvillarroel/dev/synthetic-data-for-text/notebooks/google_flan-t5-large/logs/DESCRIPCION_PROPIEDADES/trial_0/checkpoints/epoch=1-step=873392.ckpt\"\n",
        "new_model = T5Finetuner.load_from_checkpoint(NEW_MODEL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Describe 2019-09-03 precio 6850 UF tipo Departamento transacción Venta región Metropolitana de Santiago comuna Ñuñoa dormitorios 2.0 baños 2.0 constuidos 105.0 terreno nan precio_real 6850.0'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['Si quieres una increible experiencia, está la mejor oportunidad de que',\n",
              " 'no tienes todo! Ubicados just antes del centro comercial Santa Isabel.',\n",
              " 'A pasó se pude ver este hermoso punto con sus siguientes',\n",
              " 'atractivitudes: •Piscina para adultos •Club house (con',\n",
              " 'bajadas)•Gimnacio equipamiental completamente amable •Area verde',\n",
              " 'interior tipico “L”(por su formación), idealmente disfrutando al mismo',\n",
              " 'moment. Para mayor informacion contactarnos por email y/o whattapp +56',\n",
              " '9 744 84794 departamento en venta, 2 dormitorios (principal con',\n",
              " 'walking closet), bao principal completamente remodelados. Living',\n",
              " 'comedor separador de ambiente, cocina americana equipada y amuéblada',\n",
              " 'con logia incorporada, estar familiar que pude ser utilizado tambièn',\n",
              " 'para oficion de salón multiuso, piscina temperada al interior del',\n",
              " 'condominio más un quinchero mujerizado por la comunidad de los',\n",
              " 'propietarios durante todas sus tardes; accesible controlanando su',\n",
              " 'ingreso 24/7']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['VENDO DEPARTAMENTO DE 2 DORMITORIOS 2 BAÑOS CON LOGGIA Y 2',\n",
              " 'ESTACIONAMIENTOS Y 1 BODEGA DE 102 MTS2  Superficie Interior: 84.4 m2',\n",
              " 'Superficie Logia: 2.5 m2 Superficie Terraza: 15.0 m2 Superficie Total:',\n",
              " '101.9 m2  VALOR VENTA 6850 UF  FICHA EDIFICIO CHILE ESPAÑA  • Moderno',\n",
              " 'hall de acceso en doble altura y recepción finamente amoblada  • Gran',\n",
              " 'conserjería para control de acceso vehicular y peatonal  • 3 Amplios',\n",
              " 'ascensores Mitsubishi de última tecnología con sincronización',\n",
              " 'simultánea y puertas de apertura central  • Grupo electrógeno de',\n",
              " 'emergencia para ascensores y algunos espacios comunes  • Circuito',\n",
              " 'cerrado de TV y grabación de imágenes con múltiples cámaras en',\n",
              " 'accesos y ascensores  • 2 Amplias salas multiuso y sala de juegos  •',\n",
              " 'Gimnasio equipado  • Bicicenter  • Lavandería  • Piscina exterior en',\n",
              " 'primer piso  • Áreas verdes y terrazas  • Baños y cocina para',\n",
              " 'personal de servicio  • Gran cantidad de estacionamientos para visitas',\n",
              " 'en subterráneo   Saludos  LR PROPIEDADES  +56 9 93052615?']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from textwrap import wrap\n",
        "from numpy import random\n",
        "\n",
        "new_model.model.eval()\n",
        "new_model.model.cuda()\n",
        "record = df_text.sample(1).iloc[0]\n",
        "def gen_new_text(record):\n",
        "    #encoded = new_model.tokenizer.batch_encode_plus(record.text, max_length= MAX_SRC_LEN, truncation=False,padding=False,return_tensors='pt')\n",
        "    encoded = new_model.tokenizer.batch_encode_plus(record.text, \n",
        "                                                max_length= 120, \n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "    #encoded = new_model.tokenizer(\"<sep>\".join(record.text), return_tensors=\"pt\")\n",
        "    \n",
        "    max_length= random.randint(200, 600)\n",
        "    min_length = random.randint(20, max_length-5)\n",
        "    input_ids =encoded['input_ids'].cuda() \n",
        "    att = encoded['attention_mask'].cuda()\n",
        "    y_hat = new_model.model.generate(\n",
        "        inputs= input_ids,\n",
        "        attention_mask=att,\n",
        "        num_beams=1,\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "        repetition_penalty=5.5,\n",
        "        length_penalty=0.1,\n",
        "        early_stopping=True,\n",
        "        temperature=random.random(),\n",
        "        use_cache=False,\n",
        "        top_p=random.random(),\n",
        "        top_k=0,\n",
        "        do_sample=True\n",
        "    )\n",
        "    del input_ids\n",
        "    del att\n",
        "    return \" \".join([new_model.tokenizer.decode(gen_id, skip_special_tokens=True) for gen_id in y_hat])\n",
        "\n",
        "display(\" \".join(record.text))\n",
        "display(wrap(gen_new_text(record)))\n",
        "display(wrap(record.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import swifter\n",
        "data_g  = df_text.head(1000).apply(gen_new_text, axis=1)\n",
        "df_new_text = df_text.head(1000).assign(generated=data_g)\n",
        "df_new_text.to_parquet(\"m2.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as prs\n",
        "df_text.to_parquet(\"text.parquet\")\n",
        "df_p_text = prs.read_parquet(\"text.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_new_text_p(record):\n",
        "    #encoded = new_model.tokenizer.batch_encode_plus(record.text, max_length= MAX_SRC_LEN, truncation=False,padding=False,return_tensors='pt')\n",
        "    encoded = new_model.tokenizer.batch_encode_plus(record[0], \n",
        "                                                max_length= 120, \n",
        "                                                truncation=True,\n",
        "                                                padding='max_length',\n",
        "                                                return_tensors='pt')\n",
        "    #encoded = new_model.tokenizer(\"<sep>\".join(record.text), return_tensors=\"pt\")\n",
        "    \n",
        "    max_length= random.randint(200, 600)\n",
        "    min_length = random.randint(20, max_length-5)\n",
        "    input_ids =encoded['input_ids'].cuda() \n",
        "    att = encoded['attention_mask'].cuda()\n",
        "    y_hat = new_model.model.generate(\n",
        "        inputs= input_ids,\n",
        "        attention_mask=att,\n",
        "        num_beams=1,\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "        repetition_penalty=5.5,\n",
        "        length_penalty=0.1,\n",
        "        early_stopping=True,\n",
        "        temperature=random.random(),\n",
        "        use_cache=False,\n",
        "        top_p=random.random(),\n",
        "        top_k=0,\n",
        "        do_sample=True\n",
        "    )\n",
        "    del input_ids\n",
        "    del att\n",
        "    return \" \".join([new_model.tokenizer.decode(gen_id, skip_special_tokens=True) for gen_id in y_hat])\n",
        "#df_out = df_p_text.head(1000).apply(gen_new_text_p)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "syn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "339c1ec44efa671d0c0aa472f92368e3220a4c3534888d9daac06fdbcd7c45f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
