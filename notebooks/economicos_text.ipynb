{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install optuna pytorch_lightning rouge-score transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545870, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>rooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>m_built</th>\n",
       "      <th>m_size</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>address</th>\n",
       "      <th>owner</th>\n",
       "      <th>_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724951</th>\n",
       "      <td>https://www.economicos.cl/propiedades/oficina-...</td>\n",
       "      <td>Oficina de 120 metros cuadrados, con 3 oficina...</td>\n",
       "      <td>$ 800.000</td>\n",
       "      <td>Oficina o Casa Oficina</td>\n",
       "      <td>Arriendo</td>\n",
       "      <td>Biobío</td>\n",
       "      <td>Concepción</td>\n",
       "      <td>2019-08-30 22:43:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OFICINA EN ARRIENDO CENTRO DE CONCEPCION</td>\n",
       "      <td>Ohiggins 660 Concepción, Biobío</td>\n",
       "      <td>Alejandra Pavez Pardo</td>\n",
       "      <td>28.580330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403988</th>\n",
       "      <td>https://www.economicos.cl/propiedades/departam...</td>\n",
       "      <td>Viña del Mar San Martín, Vista mar, 2 dormit...</td>\n",
       "      <td>$ 400.000</td>\n",
       "      <td>Departamento</td>\n",
       "      <td>Arriendo</td>\n",
       "      <td>Valparaíso</td>\n",
       "      <td>Viña del Mar</td>\n",
       "      <td>2018-04-21 00:00:38</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Mercurio</td>\n",
       "      <td>Departamento en Arriendo en Viña Del Mar 2 dor...</td>\n",
       "      <td>Viña del Mar, Valparaíso</td>\n",
       "      <td>-1</td>\n",
       "      <td>14.821155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727247</th>\n",
       "      <td>https://www.economicos.cl/propiedades/se-vende...</td>\n",
       "      <td>PUÑIHUIL - PIEDRA RUN - ANCUD\\nSON 5000 METRO...</td>\n",
       "      <td>$ 35.000.000</td>\n",
       "      <td>Sitio o Terreno</td>\n",
       "      <td>Venta</td>\n",
       "      <td>Los Lagos</td>\n",
       "      <td>Ancud</td>\n",
       "      <td>2019-08-26 18:05:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Se vende Hermoso Terreno con Vista directa al Mar</td>\n",
       "      <td>Ancud Ancud, Los Lagos</td>\n",
       "      <td>sandra Gómez</td>\n",
       "      <td>1250.711988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "724951  https://www.economicos.cl/propiedades/oficina-...   \n",
       "403988  https://www.economicos.cl/propiedades/departam...   \n",
       "727247  https://www.economicos.cl/propiedades/se-vende...   \n",
       "\n",
       "                                              description         price  \\\n",
       "724951  Oficina de 120 metros cuadrados, con 3 oficina...     $ 800.000   \n",
       "403988  Viña del Mar San Martín, Vista mar, 2 dormit...     $ 400.000   \n",
       "727247  PUÑIHUIL - PIEDRA RUN - ANCUD\\nSON 5000 METRO...  $ 35.000.000   \n",
       "\n",
       "                 property_type transaction_type       state        county  \\\n",
       "724951  Oficina o Casa Oficina         Arriendo      Biobío    Concepción   \n",
       "403988            Departamento         Arriendo  Valparaíso  Viña del Mar   \n",
       "727247         Sitio o Terreno            Venta   Los Lagos         Ancud   \n",
       "\n",
       "          publication_date  rooms  bathrooms  m_built  m_size       source  \\\n",
       "724951 2019-08-30 22:43:09    NaN        NaN    120.0     NaN          NaN   \n",
       "403988 2018-04-21 00:00:38    2.0        2.0      NaN     NaN  El Mercurio   \n",
       "727247 2019-08-26 18:05:46    NaN        NaN      NaN  5000.0          NaN   \n",
       "\n",
       "                                                    title  \\\n",
       "724951           OFICINA EN ARRIENDO CENTRO DE CONCEPCION   \n",
       "403988  Departamento en Arriendo en Viña Del Mar 2 dor...   \n",
       "727247  Se vende Hermoso Terreno con Vista directa al Mar   \n",
       "\n",
       "                                address                   owner       _price  \n",
       "724951  Ohiggins 660 Concepción, Biobío   Alejandra Pavez Pardo    28.580330  \n",
       "403988         Viña del Mar, Valparaíso                      -1    14.821155  \n",
       "727247           Ancud Ancud, Los Lagos           sandra Gómez   1250.711988  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#.replace(to_replace=\"-1\", value=np.nan)\n",
    "df = pd.read_parquet('../datasets/economicos/synth/split/train.parquet').replace(to_replace=\"None\", value=np.nan).replace(to_replace=-1, value=np.nan)\n",
    "display(df.shape)\n",
    "CHAR_SEP = \" \"\n",
    "df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Describe', '2019-07-13 precio $ 720.000 tipo Departamento transacción Arriendo región Metropolitana de Santiago comuna Las Condes dormitorios 2.0 baños 2.0 constuidos nan terreno nan precio_real 25.7571346193874 titulo: Departamento en Arriendo en Las Condes 2 dormitorios dirección:  Las Condes, Metropolitana de Santiago'], 'target': '720.000 Dante/ Golf dos dormitorios, con/ sin muebles 991574403'}\n"
     ]
    }
   ],
   "source": [
    "def convert(row):\n",
    "    return {\n",
    "        \"text\": [\"Describe\", f\"\"\"{row.publication_date.strftime('%Y-%m-%d')}\n",
    "precio {row.price}\n",
    "tipo {row.property_type}\n",
    "transacción {row.transaction_type}\n",
    "región {row.state}\n",
    "comuna {row.county}\n",
    "dormitorios {row.rooms}\n",
    "baños {row.rooms}\n",
    "constuidos {row.m_built}\n",
    "terreno {row.m_size}\n",
    "precio_real {row._price}\n",
    "titulo: {row.title}\n",
    "dirección: {row.address}\"\"\".replace(\"\\n\", \" \")],\n",
    "        \"target\": row.description\n",
    "        }\n",
    "\n",
    "print(\n",
    "    df.sample(1).apply(convert, axis=1).iloc[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115254</th>\n",
       "      <td>[Describe, 2018-05-14 precio $ 460.000 tipo De...</td>\n",
       "      <td>Acogedor depto. de 2 dormitorios, 2 baños, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "115254  [Describe, 2018-05-14 precio $ 460.000 tipo De...   \n",
       "\n",
       "                                                   target  \n",
       "115254  Acogedor depto. de 2 dormitorios, 2 baños, co...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.DataFrame(df.apply(convert, axis=1).to_list())\n",
    "df_text.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rouge_score import rouge_scorer\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import  DataLoader, RandomSampler, SequentialSampler #Dataset,\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SEP = \" \"\n",
    "MAX_SRC_LEN = 150\n",
    "MAX_TGT_LEN = 720\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "class T5Finetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, df, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.args = args\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.args.model)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.args.model)\n",
    "        self.data = df\n",
    "        #self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.scorer = BERTScore()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _encode_text(self, text_input, target):\n",
    "      ctext = str(text_input)\n",
    "      ctext = CHAR_SEP.join(ctext.split())\n",
    "      target = str(target) #summarized text\n",
    "      target = CHAR_SEP.join(target.split())\n",
    "      source = self.tokenizer.batch_encode_plus([ctext], \n",
    "                                                max_length= MAX_SRC_LEN, \n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "      target = self.tokenizer.batch_encode_plus([target], \n",
    "                                                max_length=MAX_TGT_LEN,\n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "      y = target['input_ids']\n",
    "      target_id = y[:, :-1].contiguous()\n",
    "      target_label = y[:, 1:].clone().detach()\n",
    "      target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
    "      return source['input_ids'], source['attention_mask'], target_id, target_label\n",
    "    \n",
    "    def encode_text(self, text, target):\n",
    "        source = self.tokenizer.batch_encode_plus([text], \n",
    "                                                max_length= MAX_SRC_LEN, \n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target], \n",
    "                                                max_length=MAX_TGT_LEN,\n",
    "                                                truncation=True,\n",
    "                                                padding='max_length',\n",
    "                                                return_tensors='pt')\n",
    "        y = target['input_ids']\n",
    "        target_id = y[:, :-1].contiguous()\n",
    "        target_label = y[:, 1:].clone().detach()\n",
    "        target_label[y[:, 1:] == self.tokenizer.pad_token_id] = -100 #in case the labels are not provided, empty string\n",
    "        return source['input_ids'], source['attention_mask'], target_id, target_label\n",
    "\n",
    "        \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        source_ids, source_masks, target_ids, target_labels = [], [], [], [] \n",
    "        for _, row in self.data.iterrows():\n",
    "            source_id, source_mask, target_id, target_label = self.encode_text(row.text, row.target)\n",
    "            source_ids.append(source_id)\n",
    "            source_masks.append(source_mask)\n",
    "            target_ids.append(target_id)\n",
    "            target_labels.append(target_label)\n",
    "\n",
    "        # Convert the lists into tensors\n",
    "        source_ids = torch.cat(source_ids, dim=0)\n",
    "        source_masks = torch.cat(source_masks, dim=0)\n",
    "        target_ids = torch.cat(target_ids, dim=0)\n",
    "        target_labels = torch.cat(target_labels, dim=0)\n",
    "        # splitting the data to train, validation, and test\n",
    "        data = TensorDataset(source_ids, source_masks, target_ids, target_labels)\n",
    "        train_size, val_size = int(0.8 * len(data)), int(0.1 * len(data))\n",
    "        test_size = len(data) - (train_size + val_size)\n",
    "        self.train_dat, self.val_dat, self.test_dat = \\\n",
    "            random_split(data, [train_size, val_size, test_size])\n",
    "    \n",
    "    def forward(self, batch, batch_idx):\n",
    "        source_ids, source_mask, target_ids, target_labels = batch[:4]\n",
    "        return self.model(input_ids = source_ids, attention_mask = source_mask, \n",
    "                          decoder_input_ids=target_ids, labels=target_labels)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss, 'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
    "        out = {'val_loss': loss}\n",
    "        return {**out, 'log': out}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self(batch, batch_idx)[0]\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = sum([o['loss'] for o in outputs]) / len(outputs)\n",
    "        out = {'test_loss': loss}\n",
    "        return {**out, 'log': out}\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dat, batch_size=self.batch_size,\n",
    "                          num_workers=4, sampler=RandomSampler(self.train_dat))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dat, batch_size=self.args.bs, num_workers=4,\n",
    "                          sampler=SequentialSampler(self.val_dat))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dat, batch_size=self.args.bs, num_workers=4,\n",
    "                          sampler=SequentialSampler(self.test_dat))    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.args.lr, eps=1e-4)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0,\n",
    "            num_training_steps=self.args.max_epochs * len(self.train_dat))\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "    \n",
    "    def generate_summary(self, ctext, summ_len=150, text='', beam_search=2, repetition_penalty=2.5):\n",
    "        source_id, source_mask, target_id, target_label = self.encode_text(ctext, text)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids = source_id,\n",
    "                attention_mask = source_mask, \n",
    "                max_length=summ_len, \n",
    "                truncation=True,\n",
    "                num_beams=beam_search,\n",
    "                repetition_penalty=repetition_penalty, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            prediction = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        if len(text) > 0:\n",
    "            target = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in target_id]\n",
    "            scores = self.scorer.score(target[0], prediction[0])\n",
    "            return prediction, scores\n",
    "        else:\n",
    "            return prediction\n",
    "        \n",
    "\n",
    "    def save_core_model(self):\n",
    "        store_path = join(self.args.output, self.args.name, 'core')\n",
    "        self.model.save_pretrained(store_path)\n",
    "        self.tokenizer.save_pretrained(store_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        p = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        p.add_argument('-m', '--model', type=str, default='t5-base',\n",
    "                       help='name of the model or the path pointing to it')\n",
    "        p.add_argument('--bs', '--batch_size', type=int, default=2)\n",
    "        p.add_argument('--source_len', type=int, default=120)\n",
    "        p.add_argument('--summ_len', type=int, default=700)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_args():\n",
    "    p = ArgumentParser()\n",
    "    p.add_argument('-p', '--path', type=str,  \n",
    "                   default='/content/gdrive/My Drive/Colab Notebooks/data/text_summarization_t5/news_summary.csv',\n",
    "                  help='path to the data file')\n",
    "    p.add_argument('-o', '--output', type=str, default='/tmp/tpu-template',\n",
    "                  help='path to the output directory for storing the model')\n",
    "    p.add_argument('-n', '--name', type=str, default='google/t5-v1_1-xxl',\n",
    "                  help='this name will be used on tensorboard for the model')\n",
    "    p.add_argument('-t', '--trials', type=int, default=1,\n",
    "                  help='number of trials for hyperparameter search')\n",
    "    p.add_argument('--seed', type=int, default=0, help='randomization seed')\n",
    "    p = T5Finetuner.add_model_specific_args(p)\n",
    "    p = pl.Trainer.add_argparse_args(p)\n",
    "    args,_ = p.parse_known_args()\n",
    "    args.max_epochs = 2\n",
    "    return args\n",
    "\n",
    "def default_args():\n",
    "    p = ArgumentParser()\n",
    "    args,_ = p.parse_known_args()\n",
    "    args.max_epochs = 2\n",
    "    #args.model = \"google/flan-t5-small\"\n",
    "    args.model = \"google/flan-t5-xl\"\n",
    "    #args.model = \"google/flan-t5-large\"\n",
    "    #args.model = \"google/flan-t5-base\"\n",
    "    args.output = f\"./{args.model.replace('/','_')}_2\"\n",
    "    args.name = \"DESCRIPCION_PROPIEDADES\"\n",
    "    args.bs = 1 # batch size\n",
    "    return args\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    args = default_args()\n",
    "    # sampling the hyperparameters\n",
    "    args.lr = trial.suggest_categorical(\"lr\", [1e-6, 5e-6, 1e-5, 5e-5, 1e-4])\n",
    "    # setting up the right callbacks\n",
    "    cp_callback = pl.callbacks.ModelCheckpoint(\n",
    "        join(args.output, args.name, f\"trial_{trial.number}\", \"{epoch}\"),\n",
    "        monitor=\"val_loss\", mode=\"min\")\n",
    "    pr_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    metrics_callback = MetricsCallback()\n",
    "    summarizer = T5Finetuner(args, df_text)         # loading the model\n",
    "    trainer = pl.Trainer.from_argparse_args(      # loading the trainer\n",
    "        args, \n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        default_root_dir=args.output, gradient_clip_val=1.0,\n",
    "        #checkpoint_callback=cp_callback,\n",
    "        callbacks=[metrics_callback],\n",
    "        #early_stop_callback=pr_callback, \n",
    "        num_sanity_val_steps=-1,\n",
    "        #auto_scale_batch_size=\"power\",\n",
    "        # select TensorBoad or Wandb logger\n",
    "        logger=TensorBoardLogger(join(args.output, 'logs'), name=args.name, version=f'trial_{trial.number}')\n",
    "        )\n",
    "    trainer.fit(summarizer)                       # fitting the model\n",
    "    #trainer.test(summarizer)                      # testing the model\n",
    "    return min([x['val_loss'].item() for x in metrics_callback.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-31 17:02:00,746]\u001b[0m A new study created in memory with name: no-name-2211b714-9613-4a20-b51d-fb7a6a0f0d8a\u001b[0m\n",
      "/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torchmetrics/text/bert.py:163: UserWarning: The argument `model_name_or_path` was not specified while it is required when the default `transformers` model is used. It will use the default recommended model - 'roberta-large'.\n",
      "  warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:02:17,782]\u001b[0m Trial 0 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6efb7ffaf0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:02:32,720]\u001b[0m Trial 1 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087cb7c0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:02:47,285]\u001b[0m Trial 2 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087a93d0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:03:01,789]\u001b[0m Trial 3 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087cbe80> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:03:16,200]\u001b[0m Trial 4 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087a9b80> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:03:30,724]\u001b[0m Trial 5 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087a97c0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:03:45,241]\u001b[0m Trial 6 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087cbeb0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:03:59,785]\u001b[0m Trial 7 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087cbf40> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:04:14,421]\u001b[0m Trial 8 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f0a54e4f0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:04:29,078]\u001b[0m Trial 9 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f087a9e80> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:04:43,782]\u001b[0m Trial 10 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f0a54eeb0> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:04:58,308]\u001b[0m Trial 11 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f0a54e220> could not be cast to float.\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-12-31 17:05:12,835]\u001b[0m Trial 12 failed because of the following error: The value <pytorch_lightning.trainer.trainer.Trainer object at 0x7f6f0a54ed90> could not be cast to float.\u001b[0m\n",
      "\u001b[33m[W 2022-12-31 17:05:15,275]\u001b[0m Trial 13 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2051632/2541432343.py\", line 42, in optuna_objective\n",
      "    summarizer = T5Finetuner(args, df_text)         # loading the model\n",
      "  File \"/tmp/ipykernel_2051632/2439732350.py\", line 12, in __init__\n",
      "    self.model = T5ForConditionalGeneration.from_pretrained(self.args.model)\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 2276, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1486, in __init__\n",
      "    self.encoder = T5Stack(encoder_config, self.shared)\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 848, in __init__\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 848, in <listcomp>\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 634, in __init__\n",
      "    self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 564, in __init__\n",
      "    self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 348, in __init__\n",
      "    self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 101, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 107, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/home/gvillarroel/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/init.py\", line 412, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study()\n\u001b[0;32m----> 2\u001b[0m study\u001b[39m.\u001b[39;49moptimize(optuna_objective, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [9], line 42\u001b[0m, in \u001b[0;36moptuna_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     40\u001b[0m pr_callback \u001b[39m=\u001b[39m PyTorchLightningPruningCallback(trial, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m metrics_callback \u001b[39m=\u001b[39m MetricsCallback()\n\u001b[0;32m---> 42\u001b[0m summarizer \u001b[39m=\u001b[39m T5Finetuner(args, df_text)         \u001b[39m# loading the model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer\u001b[39m.\u001b[39mfrom_argparse_args(      \u001b[39m# loading the trainer\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     args, \n\u001b[1;32m     45\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     logger\u001b[39m=\u001b[39mTensorBoardLogger(join(args\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m'\u001b[39m), name\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mname, version\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrial_\u001b[39m\u001b[39m{\u001b[39;00mtrial\u001b[39m.\u001b[39mnumber\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\n",
      "Cell \u001b[0;32mIn [8], line 12\u001b[0m, in \u001b[0;36mT5Finetuner.__init__\u001b[0;34m(self, args, df, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[0;32m---> 12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m df\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/modeling_utils.py:2276\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2275\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2276\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2278\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2279\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1486\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1484\u001b[0m encoder_config\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m encoder_config\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m T5Stack(encoder_config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared)\n\u001b[1;32m   1488\u001b[0m decoder_config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)\n\u001b[1;32m   1489\u001b[0m decoder_config\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:848\u001b[0m, in \u001b[0;36mT5Stack.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m embed_tokens\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    847\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 848\u001b[0m     [T5Block(config, has_relative_attention_bias\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_layers)]\n\u001b[1;32m    849\u001b[0m )\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:848\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m embed_tokens\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    847\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 848\u001b[0m     [T5Block(config, has_relative_attention_bias\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(i \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_layers)]\n\u001b[1;32m    849\u001b[0m )\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:634\u001b[0m, in \u001b[0;36mT5Block.__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    633\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[0;32m--> 634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(T5LayerSelfAttention(config, has_relative_attention_bias\u001b[39m=\u001b[39;49mhas_relative_attention_bias))\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(T5LayerCrossAttention(config))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:564\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, has_relative_attention_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    563\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSelfAttention \u001b[39m=\u001b[39m T5Attention(config, has_relative_attention_bias\u001b[39m=\u001b[39;49mhas_relative_attention_bias)\n\u001b[1;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:348\u001b[0m, in \u001b[0;36mT5Attention.__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39m# Mesh TensorFlow initialization to avoid scaling before softmax\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_dim, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/syn/lib/python3.9/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(optuna_objective, show_progress_bar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "339c1ec44efa671d0c0aa472f92368e3220a4c3534888d9daac06fdbcd7c45f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
