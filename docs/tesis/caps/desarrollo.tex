\chapter{Desarrollo}
\section{Recursos disponibles}
\subsection{Conjuntos de datos}
A continuación se listan y detallan los conjuntos de datos utilizados en los experimentos.

\subsubsection{King County}
El conjunto de datos King County \cite{kaggle_house_2015} contiene información sobre precios de venta y características de 21,613 viviendas en Seattle y King County de los años 2014 y 2015. El conjunto de datos incluye información como el número de habitaciones, el número de baños, la superficie del terreno y la superficie construida, así como información sobre la ubicación de la propiedad, como la latitud y la longitud. Este conjunto de datos es comúnmente utilizado para tareas de regresión y predicción de precios de viviendas. Sus campos se listan en \fullref{data-county}.
 
\begin{table}[H]
	\centering
	\caption{Conjunto de datos King County}
	\label{data-county}
    \begin{tabular}{|l|m{30em}|}
        \hline
        \rowcolor[gray]{0.8}
        Variable & Descripción \\
        \hline
        id & Identificación \\
        \hline
        date & Fecha de venta \\
        \hline
        price & Precio de venta \\
        \hline
        bedrooms & Número de dormitorios \\
        \hline
        bathrooms & Número de baños \\
        \hline
        sqft\_liv & Tamaño del área habitable en pies cuadrados \\
        \hline
        sqft\_lot & Tamaño del terreno en pies cuadrados \\
        \hline
        floors & Número de pisos \\
        \hline
        waterfront & '1' si la propiedad tiene vista al mar, '0' si no \\
        \hline
        view & Índice del 0 al 4 de la calidad de la vista de la propiedad \\
        \hline
        condition & Condición de la casa, clasificada del 1 al 5 \\
        \hline
        grade & Clasificación por calidad de construcción que se refiere a los tipos de materiales utilizados y la calidad de la mano de obra. Los edificios de mejor calidad (grado más alto) cuestan más construir por unidad de medida y tienen un valor más alto. Información adicional en: KingCounty \\
        \hline
        sqft\_above & Pies cuadrados sobre el nivel del suelo \\
        \hline
        sqft\_basmt & Pies cuadrados debajo del nivel del suelo \\
        \hline
        yr\_built & Año de construcción \\
        \hline
        yr\_renov & Año de renovación. '0' si nunca se ha renovado \\
        \hline
        zipcode & Código postal de 5 dígitos \\
        \hline
        lat & Latitud \\
        \hline
        long & Longitud \\
        \hline
        sqft\_liv15 & Tamaño promedio del espacio habitable interior para las 15 casas más cercanas, en pies cuadrados \\
        \hline
        sqft\_lot15 & Tamaño promedio de los terrenos para las 15 casas más cercanas, en pies cuadrados \\
        \hline
        Shape\_leng & Longitud del polígono en metros \\
        \hline
        Shape\_Area & Área del polígono en metros \\
        \hline
    \end{tabular}
\end{table}  


\subsubsection{Economicos}
Economicos.cl es un sitio web chileno que se dedica a la publicación de avisos clasificados en línea, principalmente en las categorías de bienes raíces, vehículos, empleos, servicios y productos diversos.
El conjunto de datos corresponde a un \emph{Web Scraping} realizado en 2020, contiene 22.059 observaciones.

\begin{table}[H]
	\centering
	\caption{Conjunto de datos Economicos.cl}
	\label{data-economicos}
    \begin{tabular}{|l|m{30em}|}
        \hline
        \rowcolor[gray]{0.8}
        Variable & Descripción \\
        \hline
        url & URL de la publicación \\
        \hline
        Descripción & Descripción de la publicación \\
        \hline
        price & Precio de venta, en dolares, UF o pesos \\
        \hline
        property\_type & Tipo de propiedad: Casa, Departamento, ETC \\
        \hline
        transaction\_type & Tipo de transactión\: Arriendo, Venta \\
        \hline
        state & Región de la publicación \\
        \hline
        county & Comuna de la publicación \\
        \hline
        publication\_date & Día de la publicación \\
        \hline
        rooms & Número de dormitorios \\
        \hline
        bathrooms & Número de baños \\
        \hline
        m\_built & Tamaño del área habitable en metros cuadrados \\
        \hline
        m\_size & Tamaño del terreno en metros cuadrados \\
        \hline
        source & Diario de la publicación \\
        \hline
        title & Titulo de la publicación \\
        \hline
        address & Dirección de la publicación \\
        \hline
        owner & Publicante \\
        \hline
        \_price & Precio traspasado a UF \\
        \hline
    \end{tabular}
\end{table}  



\subsection{Computación y Software}

Para llevar a cabo los experimentos, se utilizó un computador con las siguientes especificaciones técnicas, como se muestra en la \fullref{tabla-componentes-pc}. El procesador empleado fue un AMD Ryzen 9 7950X 16-Core Processor, con cuatro modulos de 32 GB para una memoria total de 128 GB DDR5. La tarjeta gráfica empleada fue una NVIDIA GeForce RTX 4090, y se contó con dos discos duros de 500 GB SSD. La utilización de un equipo con estas características permitió una ejecución eficiente de los modelos de generación de datos, asegurando la viabilidad de los experimentos. Es importante destacar que la elección de los componentes del computador fue cuidadosamente considerada para asegurar que los resultados obtenidos no se vieran limitados por un hardware insuficiente.

En relación al software utilizado, se trabajó con el sistema operativo Ubuntu 20.04.2 LTS y se empleó el lenguaje de programación Python 3.9.5 para el desarrollo de los modelos de generación de datos. Se utilizaron diversas bibliotecas, incluyendo DVC, SDV y PyTorch, cuya lista completa se puede encontrar en el \href{https://github.com/gvillarroel/synthetic-data-for-text/blob/main/freeze.txt}{repositorio en Github}. La elección de estas herramientas se basó en la compatibilidad con el modelo TabDDPM, el cual fue utilizado en algunos de los experimentos.

\begin{table}[H]
	\centering
	\caption{Computador Usado}
	\label{tabla-componentes-pc}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor[gray]{0.8}
        Componente & Descripción \\
        \hline
        Procesador & AMD Ryzen 9 7950X 16-Core Processor \\
        \hline
        Memoria RAM & 128 GB DDR5 \\
        \hline
        Tarjeta gráfica & NVIDIA GeForce RTX 4090 \\
        \hline
        Disco duro & 1 TB SSD \\
        \hline
      \end{tabular}        
\end{table}  

El código fuente de los modelos de generación de datos, así como los scripts de análisis y visualización de los resultados, se encuentra disponible en un repositorio público de Github:
\href{https://github.com/gvillarroel/synthetic-data-for-text}{gvillarroel/synthetic-data-for-text}

\section{Desarrollo del flujo de procesamiento}
A continuación se describe el flujo de procesamiento utilizado para generar nuevos datos sintéticos. Este flujo se basa en el propuesto por Synthetic Data Vault (SDV), con algunas modificaciones para guardar etapas intermedias.

SDV es un ecosistema de bibliotecas de generación de datos sintéticos que permite a los usuarios aprender conjuntos de datos de una sola tabla, de múltiples tablas y de series de tiempo, y luego generar nuevos datos sintéticos con las mismas propiedades estadísticas y el mismo formato que los conjuntos de datos originales. Para ello, SDV utiliza diferentes técnicas, como modelos generativos y redes neuronales, para aprender la distribución subyacente de los datos y generar nuevos datos que sigan dicha distribución \cite{kotelnikov_overview_nodate, patki_synthetic_2016}.

A continuación se describe el proceso de generación de datos sintéticos para una tabla única utilizando la biblioteca Synthetic Data Vault (SDV), seguido de las modificaciones realizadas para extender el proceso y agregar nuevos modelos.


\newpage
En la \fullref{process-sdv} se muestran los pasos necesarios para generar un conjunto de datos sintéticos utilizando SDV:

\begin{figure}[H]
	\centering
	\includesvg[scale=.2,inkscapelatex=false]{../dfd/images/processor.svg}
	\caption{Proceso para generar datos sintéticos con SDV}
	\label{process-sdv}
\end{figure}

\begin{enumerate}
    \item \textbf{Create Metadata}: Se crea un diccionario que define los campos del conjunto de datos y los tipos de datos que posee. Esto permite a SDV aprender la estructura del conjunto de datos original y utilizarla para generar nuevos datos sintéticos con la misma estructura.
    \item \textbf{Create Model}: Se selecciona el modelo de generación de datos a utilizar. SDV ofrece varios modelos, incluyendo GaussianCopula, CTGAN, CopulaGAN y TVAE, que se adaptan a diferentes tipos de datos y distribuciones.
    \item \textbf{Fit Model}: El modelo seleccionado se entrena con el conjunto de datos original para aprender sus distribuciones y patrones estadísticos.
    \item \textbf{Generate Synthetic Dataset}: Con el modelo ya entrenado, se generan nuevos datos sintéticos con la misma estructura y características estadísticas que el conjunto original. Este nuevo conjunto de datos puede ser utilizado para diversas aplicaciones, como pruebas de software o análisis de datos sensibles.
\end{enumerate}

Es importante destacar que el proceso de generación de datos sintéticos con SDV es escalable y puede utilizarse con conjuntos de datos de una sola tabla, múltiples tablas y series de tiempo. Además, en este proyecto se realizaron algunas modificaciones al flujo para extender el proceso y permitir la inyección de nuevos modelos.
    
\newpage
En el proceso de generación de datos sintéticos con SDV extendido, se incluyen dos nuevas etapas para poder guardar los modelos intermedios y los resultados de la evaluación. El proceso completo se muestra en la \fullref{process-sdv-2} y consta de los siguientes pasos:

\begin{enumerate}
    \item \textbf{Create Metadata}: Crea un diccionario que define los campos del conjunto de datos y los tipos de datos que posee.
    \item \textbf{Create Model}: Se selecciona el modelo a utilizar. SDV permite GaussianCopula, CTGAN, CopulaGAN y TVAE.
    \item \textbf{Fit Model}: El modelo seleccionado toma el conjunto original para entrenar el modelo y aprender sus distribuciones.
    \item \textbf{Save Model}: El modelo entrenado se guarda en un archivo para su uso posterior.
    \item \textbf{Generate Synthetic Dataset}: Genera un nuevo conjunto de datos usando el modelo entrenado.
    \item \textbf{Evaluate \& Save Metrics}: Evalúa y guarda el conjunto de datos sintético generado mediante métricas como la correlación, el error absoluto medio y el error cuadrático medio.
\end{enumerate}


\begin{figure}[H]
	\centering
	\includesvg[scale=.15,inkscapelatex=false]{../dfd/images/processor_edited.svg}
	\caption{Proceso para generar datos sintéticos completo}
	\label{process-sdv-2}
\end{figure}

Con estas nuevas etapas, se pueden guardar los modelos intermedios y los resultados de la evaluación, lo que permite una mayor flexibilidad en el proceso y la capacidad de utilizar los modelos y los resultados en posteriores experimentos.



\section{Modelos}

Los modelos de generación de datos tabulares utilizan como base la metodología propuesta por \emph{Synthetic Data Vault} (SDV), mientras que para los modelos de generación de texto se utiliza la biblioteca Hugging Face para cargar, realizar \emph{fine-tuning} con nuevas tareas y evaluar el modelo basado en mT5.

\subsection{Modelos para datos tabulares}
Para que un modelo pueda ser utilizado con el SDV, es necesario que implemente los siguientes métodos:
\begin{enumerate}
    \item \textbf{load}: Carga el modelo desde un archivo
    \item \textbf{fit}: Entrena el modelo, utilizando un pandas dataframe como entrada
    \item \textbf{save}: Guarda el modelo en un archivo
    \item \textbf{sample}: Genera un conjunto de registros nuevos utilizando el modelo entrenado.
\end{enumerate}

Como consideración adicional, se recomienda ejecutar el proceso utilizando un script en lugar de un notebook, ya que se ha observado que el notebook puede fallar con algunos modelos debido a limitaciones de memoria. A continuación, se detallan los pasos a seguir para la ejecución del proceso:
\begin{enumerate}
\item Crear un archivo de configuración que contenga la información necesaria para la generación de datos sintéticos, como la ruta del conjunto de datos original y la configuración de los modelos a utilizar.
\item Crear un script que cargue la configuración, ejecute el proceso de generación de datos sintéticos y guarde el conjunto de datos sintético resultante.
\item Ejecutar el script creado en el paso anterior.
\end{enumerate}
De esta manera, se puede ejecutar el proceso de generación de datos sintéticos de forma automatizada y con una mayor capacidad de procesamiento, lo que puede mejorar el desempeño del proceso y reducir los tiempos de ejecución. Vea \nameref{a-2}

La clase \emph{Synthetic} es una implementación que permite configurar los modelos a utilizar en el proceso de generación de datos sintéticos. Esta clase encapsula los métodos comunes de los modelos, como \emph{load}, \emph{fit}, \emph{save} y \emph{sample}, permitiendo así una configuración general de las entradas y la selección de modelos.

En el ejemplo mostrado en el Código \fullref{code-economicos-synthetic}, se instancia la clase \emph{Synthetic} con un pandas dataframe previamente pre-procesado. Se especifican las columnas que se considerarán como categorías, las que se considerarán como texto y las que se excluirán del análisis. Además, se indica el directorio donde se guardarán los archivos temporales, se seleccionan los modelos a utilizar, se establece el número de registros sintéticos deseados y se define una columna objetivo para realizar pruebas con machine learning y estratificar los conjuntos parciales de datos que se utilizarán. De esta manera, se configura de manera flexible el proceso de generación de datos sintéticos según las necesidades específicas del usuario.
\begin{listing}[H]
\inputminted[
    firstline=45, lastline=54
    ]{python}{../../notebooks/economicos_train.py}
\caption{Instanciando clase Synthetic}
\label{code-economicos-synthetic}
\end{listing}

La \fullref{synthetic-input} presenta las opciones para la instancia de la clase \emph{Synthetic}:

\begin{table}[H]
\centering
\caption{Variables de entrada para \emph{Synthetic}}
\label{synthetic-input}
\begin{tabular}{|l|m{25em}|}
\hline
\rowcolor[gray]{0.8}
\textbf{Variable} & \textbf{Descripción} \\
\hline
df & Pandas DataFrame a utilizar \\
\hline
Id & Nombre de la columna a ser usada como identificadora \\
\hline
category\_columns & Listado de columnas categóricas \\
\hline
text\_columns & Listado de columnas de texto \\
\hline
exclude\_columns & Listado de columnas que deben ser excluidas \\
\hline
synthetic\_folder & Carpeta donde se guardarán los documentos intermedios y finales \\
\hline
models & Listado de modelos a utilizar \\
\hline
n\_sample & Número de registros a generar \\
\hline
target\_column & Columna a utilizar como objetivo para modelos de machine learning en las evaluaciones y separación cuando se deba estratificar los campos. \\
\hline
\end{tabular}
\end{table}

En la \fullref{modelos-tab-soportados} se detallan los modelos actualmente soportados en la clase \emph{Synthetic} y su origen.

\begin{table}[H]
	\centering
	\caption{Modelos Tabulares Soportados}
	\label{modelos-tab-soportados}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor[gray]{0.8}
        Nombre Modelo & Fuente \\
        \hline
        copulagan & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        tvae & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        gaussiancopula & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        ctgan & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        smote-enc & tabDDPM \cite{akim_tabddpm_2023} \\
        \hline
        tddpm\_mlp & tabDDPM \cite{akim_tabddpm_2023} \\
        \hline
      \end{tabular}        
\end{table} 

Al ejecutar el script de generación de datos sintéticos, se crearán múltiples archivos en una carpeta. En la \fullref{synth-folders} se muestra un ejemplo de los archivos generados y su formato. El nombre del modelo utilizado se indica en el campo \textbf{<model>}, y en caso de haberse aplicado \emph{Differential Privacy} para generar una versión con ruido, se añade el sufijo \textbf{\_noise} al nombre del modelo. El campo \textbf{<n\_sample>} indica el número de registros sintéticos generados, y finalmente el campo \textbf{<type\_comparison>} especifica si se trata de una comparación entre los datos sintéticos y los datos de entrenamiento (\emph{Synthetic vs Train}, abreviado como ST) o entre los datos sintéticos y los datos de validación (\emph{Synthetic vs Hold}, abreviado como SH). Adicionalmente se encuentran los archivos de esquema (\emph{metadata.json} y \emph{metadata\_noise.json}) y una separación del dataset inicial en el conjunto de entrenamiento y test (hold).
\begin{figure}[H]
	\centering
	\includesvg[scale=1,inkscapelatex=false]{../dfd/images/synth_tree.svg}
	\caption{Carpetas y archivos esperados generados por \emph{Synthetic}}
	\label{synth-folders}
\end{figure}

\newpage
\subsection{Modelos para textos}
Como se mencionó anteriormente, se utilizó el modelo \textbf{mT5} que se entrenó para una nueva tarea utilizando la estrategia presentada en el artículo \emph{Text-to-Text Pre-Training for Data-to-Text Tasks} \cite{kale_text--text_2020}. Para ilustrar el proceso, se presenta un ejemplo del texto pre-procesado, el segmento de la pregunta y la respuesta esperada para un registro del conjunto de datos \emph{economicos.cl}.

\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Input]
<fecha, 2022-01-01>\\
<precio, \$ 105.000.000>\\
<tipo, Departamento>\\
<transacci\'on, Venta>\\
<regi\'on, Metropolitana de Santiago>\\
<comuna, Santiago>\\
<dormitorios, 3.0>\\
<ba\~nos, 3.0>\\
<construidos, 47.0>\\
<terreno, 47.0>\\
<precio\_real, 3387.4540447373292>\\
<titulo, Departamento en Venta en Santiago 3 dormitorios 1 ba\~no>\\
<direcci\'on, DEPARTAMENTO EN EL CORAZON DE LO BARNECHEA Santiago, Metropolitana de Santiago>
\end{tcolorbox}
\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Tarea]
descripci\'on de esta publicaci\'on
\end{tcolorbox}
\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Respuesta esperada]
\footnotesize
Kazona Propiedades Vende Departamento de 47m2, 3 dormitorios, 1 ba\~no, cocina, living comedor , Paredes con Cer\'amica y Tabiquer\'ia en techo con madera barnizada timbrada, ventanas nuevas de PVC y vidrio termolaminado, sistema el\'ectrico actualizado, departamento ubicado en el 3er nivel (sin ascensor) , bajo gasto com\'un. Excelentes conectividades y ubicaci\'on en Pleno Centro De Lo Barnechea, como colegios privados y p\'ublicos, supermercados, Mall Portal La Dehesa, locomoci\'on, entre otros.\\
Podemos destacar de la propiedad:\\
Pleno Centro Lo Barnechea\\
100 metros de locomoci\'on a Escuela Militar , Bilbao, Stgo Centro, Mapocho\\
200 metros colegios Montessori Nido de \'Aguila, San Rafael , otros\\
200 metros Mall Portal La Dehesa\\
200 metros Sta. Isabel\\
300 metros carabineros\\
Gastos comunes bajos  \$10.000\\
Estacionamiento comunitario\\
No paga contribuciones\\
Contactanos al telefono Kazona 569 56031154
\end{tcolorbox}

\section{Obtención de Métricas}

Se han automatizado la mayoría de las métricas para evaluar los conjuntos de datos sintéticos mediante el módulo \emph{metrics}. Estas métricas se aplican a los tres conjuntos de datos para su evaluación, lo que permite calcular estadísticas y comparativas para el conjunto de datos real utilizado para el entrenamiento (train dataset), el conjunto de datos reservado para la evaluación (hold) y el conjunto de datos sintético generado por los diferentes modelos (synthetic). Se pueden recolectar ejecutando el ejemplo de código proporcionado en Código \fullref{code-economicos-metrics}.

\begin{listing}[H]
\inputminted[
    firstline=33, lastline=34
    ]{python}{../../notebooks/economicos_run-a.py}
\caption{Obtención de métricas en economicos\_run-a.py}
\label{code-economicos-metrics}
\end{listing}

En la \fullref{metricas-numericas} se muestra las metricas recolectadas para campos numericos.

\begin{longtable}{|m{10em}|m{25em}|}
    \caption{Metricas para campos numericos} 
    \label{metricas-numericas} \\
    \hline
    \rowcolor[gray]{0.8}
    Campo & Ejemplos \\
    \hline
    \endfirsthead
    
    \hline
    \rowcolor[gray]{0.8}
    Campo & Ejemplos \\
    \hline
    \endhead
    
    \hline \multicolumn{2}{|r|}{{Continúa en la siguiente página}} \\ \hline
    \endfoot
    
    \hline \hline
    \endlastfoot
    
    \hline
    Nombre del campo (name) & sqft\_living \\ \hline
    Valores del Top 5 (top5) & [1400 1300 1720 1250 1540] \\ \hline
    Frecuencia Top 5 (top5\_frec) & [109 107 106 106 105] \\ \hline
    Probabilidades de Top 5 (top5\_prob) & [0.00630422 0.00618855 0.00613071 0.00613071 0.00607287] \\ \hline
    Elementos observados (nobs) & 17290 \\ \hline
    Nulos (missing) & 0 \\ \hline
    Promedio (mean) & 2073.894910 \\ \hline
    Desviación Estándar (std) & 907.297963 \\ \hline
    Error estándar de la media (std\_err) & 6.900053 \\ \hline
    Intervalo de confianza superior (upper\_ci) & 2087.418766 \\ \hline
    Intervalo de confianza inferior (lower\_ci) & 2060.371055 \\ \hline
    Rango intercuartílico (iqr) & 1110 \\ \hline
    Rango intercuartílico normalizado (iqr\_normal) & 822.844231 \\ \hline
    Desviación absoluta de la mediana (mad) & 693.180169 \\ \hline
    Desviación absoluta de la mediana normalizada (mad\_normal) & 868.772506 \\ \hline
    Coeficiente de variación (coef\_var) & 0.437485 \\ \hline
    Rango (range) & 11760 \\ \hline
    Valor máximo (max) & 12050 \\ \hline
    Valor mínimo (min) & 290 \\ \hline
    Sesgo (skew) & 1.370859 \\ \hline
    Curtosis (kurtosis) & 7.166622 \\ \hline
    Test de normalidad de Jarque-Bera (jarque\_bera) & 17922.347382 \\ \hline
    Valor p del test de normalidad de Jarque-Bera (jarque\_bera\_pval) & 0 \\ \hline
    Moda (mode) & 1400 \\ \hline
    Frecuencia de la moda (mode\_freq) & 0.006304 \\ \hline
    Mediana (median) & 1910 \\ \hline
    Percentil 0.1\% & 522.890000 \\ \hline
    Percentil 1\% & 720 \\ \hline
    Percentil 5\% & 940 \\ \hline
    Percentil 25\% & 1430 \\ \hline
    Percentil 75\% & 2540 \\ \hline
    Percentil 95\% & 3740 \\ \hline
    Percentil 99\% & 4921.100000 \\ \hline
    Percentil 99.9\% & 6965.550000 \\ \hline
\end{longtable}

\newpage
En la \fullref{metricas-categoricas} se muestran los datos calculados para campos categóricos.

\begin{table}[H]
    \centering
    \caption{Métricas para campos categóricos}
    \label{metricas-categoricas}
    \begin{tabular}{|m{10em}|m{25em}|}
        \hline
        \rowcolor[gray]{0.8}
        Nombre del campo (name) & waterfront \\ \hline
        Valores del Top 5 (top5) & [0 1] \\ \hline
        Frecuencia Top 5 (top5\_freq) & [17166   124] \\ \hline
        Probabilidades de Top 5 (top5\_prob) & [0.99282822 0.00717178] \\ \hline
        Elementos observados (nobs) & 17290.0 \\ \hline
        Nulos (missing) & 17290.0 \\ \hline
    \end{tabular}
\end{table}
\newpage
En el Código \fullref{codigo-show-score}, se muestra cómo se calcula y se muestra el Score promedio para una selección específica de modelos. El código utiliza la función "sort\_values" para ordenar los resultados en orden descendente según el puntaje. Luego, se filtran los resultados para incluir solo los modelos seleccionados y las columnas que muestran el puntaje y la Distancia al registro más cercano (DCR) en los tres umbrales \emph{Synthetic vs Train} (ST), \emph{Synthetic vs Hold} (SH) y \emph{Train vs Hold} TH.
\begin{listing}[H]
    \begin{minted}[linenos=true,frame=lines,framesep=2mm,baselinestretch=1.2]{python}
avg = syn.scores[syn.scores["type"] == "avg"]
avg.sort_values("score", ascending=False).loc[ ["tddpm_mlp_21613","smote-enc_21613","gaussiancopula_noise_21613","tvae_21613", "tvae_noise_21613","gaussiancopula_21613", "copulagan_noise_21613","copulagan_21613","ctgan_noise_21613","ctgan_21613"], ["score", "DCR ST 5th", "DCR SH 5th", "DCR TH 5th"]]
    \end{minted}
\caption{Mostrando Scores Promedios Calculados}
\label{codigo-show-score}
\end{listing}

El Score calculado se obtiene a través de SDV y se basa en cuatro métricas: KSComplement, TVComplement que conforman \emph{Column Shapes}, ContingencySimilarity y CorrelationSimilarity conforman \emph{Column Pair Trends}. Además, para mostrar los resultados, se proporciona un ejemplo de código en el Código \fullref{codigo-show-score} y un ejemplo de resultado en la \fullref{tabla-show-score}.

\begin{table}[H]
    \centering
    \caption{Ejemplo de scores promedios}
    \label{tabla-show-score}
    \begin{tabular}{|m{10em}|m{5em}|m{5em}|r|r|r|r|}
        \hline
        \rowcolor[gray]{0.8}
        Nombre & Column Pair Trends & Column Shapes & \textbf{Score} $\downarrow$ & DCR ST& DCR SH& DCR TH\\ \hline
        ntddpm\_mlp\_21613 & 0.954 & 0.971 & 0.962 & 0.084 & 0.104 & 0.035 \\ \hline
        nsmote-enc\_21613 & 0.941 & 0.967 & 0.954 & 0.058 & 0.090 & 0.035 \\ \hline
        \textbf{<model>}\_\textbf{<use\_noise>} \_\textbf{<n\_sample>} & 0.941 & 0.967 & 0.954 & 0.058 & 0.090 & 0.035 \\ \hline
    \end{tabular}
\end{table}

%\begin{table}[H]
%    \centering
%    \caption{Scores promedios de un Modelo}
%    \label{tabla-show-score}
%    \begin{tabular}{|l|r|r|r|r|}
%        \hline
%        \rowcolor[gray]{0.8}
%        name & score & DCR ST 5th & DCR SH 5th & DCR TH 5th \\ \hline
%        ntddpm\_mlp\_21613 & 0.962789 & 0.084187 & 0.104472 & 0.035750 \\ \hline
%        nsmote-enc\_21613 & 0.954592 & 0.058759 & 0.090209 & 0.035750 \\ \hline
%        ngaussiancopula\_noise\_21613 & 0.827655 & 0.209163 & 0.229880 & 0.035750 \\ \hline
%        ntvae\_21613 & 0.819230 & 0.108154 & 0.133609 & 0.035750 \\ \hline
%        ntvae\_noise\_21613 & 0.814853 & 0.121998 & 0.139551 & 0.035750 \\ \hline
%        ngaussiancopula\_21613 & 0.812633 & 0.206765 & 0.226026 & 0.035750 \\ \hline
%        ncopulagan\_noise\_21613 & 0.803317 & 0.230017 & 0.253036 & 0.035750 \\ \hline
%        ncopulagan\_21613 & 0.780299 & 0.219406 & 0.242047 & 0.035750 \\ \hline
%        nctgan\_noise\_21613 & 0.760218 & 0.207462 & 0.230596 & 0.035750 \\ \hline
%        nctgan\_21613 & 0.741038 & 0.210415 & 0.232722 & 0.035750 \\ \hline
%    \end{tabular}
%\end{table}