\chapter{Desarrollo}
\label{chap:desarrollo}
La generación de datos sintéticos es una práctica en rápida evolución con implicaciones significativas para el campo de la inteligencia artificial. Este capítulo se centra en proporcionar una visión detallada del desarrollo y la implementación este proceso, con el fin de la generación de dichos datos.

En primer lugar, se examinan los recursos disponibles que forman la base de este estudio. Se describen dos conjuntos de datos principales: uno derivado de la información de los precios de las viviendas de King County y otro de Economicos.cl, un portal chileno de anuncios clasificados. Además, se proporciona información detallada sobre el equipo informático y el software empleado en este estudio.

El núcleo de este capítulo se centra en la explicación del desarrollo de un proceso de generación de datos sintéticos. Este proceso se basa en la metodología propuesta por Synthetic Data Vault (SDV) y se extiende para incluir etapas intermedias de almacenamiento de modelos y resultados de evaluación. Se proporciona una descripción detallada de cada paso del proceso, desde la creación de metadatos hasta la generación del conjunto de datos sintéticos.

Para concluir, se abordan los métodos para la evaluación de los conjuntos de datos sintéticos generados. Esto incluye la descripción de cómo se obtienen y calculan las métricas, con un ejemplo de cómo calcular y visualizar el \emph{score} promedio para una selección específica de modelos. Este capítulo prepara el terreno para un análisis profundo de los resultados obtenidos a través de esta metodología, que se presentará en los capítulos siguientes.

\section{Recursos disponibles}
\label{sec:recursos}
\subsection{Conjuntos de datos}
\label{subsec:datos}
A continuación se describen y detallan las bases de datos utilizadas en los experimentos.

\subsubsection{King County}
La base de datos de King County \cite{kaggle_house_2015} contiene notificación sobre precios de venta y características de 21,613 viviendas en Seattle y King County de los años 2014 y 2015. La base incluye datos como el número de habitaciones, el número de baños, la superficie del terreno y la superficie construida, así como detalles sobre la ubicación de la propiedad, como la latitud y la longitud. Este paquete de datos es comúnmente utilizado para tareas de regresión y predicción de precios de viviendas. Sus campos se describen en la Tabla \ref{data-county}.
 
\begin{table}[H]
	\centering
	\caption{Conjunto de datos King County}
	\label{data-county}
    \begin{tabular}{|l|m{30em}|}
        \hline
        \rowcolor[gray]{0.8}
        Variable & Descripción \\
        \hline
        id & Identificación \\
        \hline
        date & Fecha de venta \\
        \hline
        price & Precio de venta \\
        \hline
        bedrooms & Número de dormitorios \\
        \hline
        bathrooms & Número de baños \\
        \hline
        sqft\_liv & Tamaño del área habitable en pies cuadrados \\
        \hline
        sqft\_lot & Tamaño del terreno en pies cuadrados \\
        \hline
        floors & Número de pisos \\
        \hline
        waterfront & '1' si la propiedad tiene vista al mar, '0' si no \\
        \hline
        view & Índice del 0 al 4 de la calidad de la vista de la propiedad \\
        \hline
        condition & Condición de la casa, clasificada del 1 al 5 \\
        \hline
        grade & Clasificación por calidad de construcción que se refiere a los tipos de materiales utilizados y la calidad de la mano de obra. Los edificios de mejor calidad (grado más alto) cuestan más construir por unidad de medida y tienen un valor más alto. Información adicional en: KingCounty \\
        \hline
        sqft\_above & Pies cuadrados sobre el nivel del suelo \\
        \hline
        sqft\_basmt & Pies cuadrados debajo del nivel del suelo \\
        \hline
        yr\_built & Año de construcción \\
        \hline
        yr\_renov & Año de renovación. '0' si nunca se ha renovado \\
        \hline
        zipcode & Código postal de 5 dígitos \\
        \hline
        lat & Latitud \\
        \hline
        long & Longitud \\
        \hline
        sqft\_liv15 & Tamaño promedio del espacio habitable interior para las 15 casas más cercanas, en pies cuadrados \\
        \hline
        sqft\_lot15 & Tamaño promedio de los terrenos para las 15 casas más cercanas, en pies cuadrados \\
        \hline
        Shape\_leng & Longitud del polígono en metros \\
        \hline
        Shape\_Area & Área del polígono en metros \\
        \hline
    \end{tabular}
\end{table}  


\subsubsection{Económicos}
Economicos.cl es un portal web chileno que se especializa en la publicación de anuncios clasificados en línea, enfocándose en las categorías de bienes raíces, vehículos, empleos, servicios y productos variados. 
La base de datos se originó de un \emph{Web Scraping} ejecutado en 2020, y contiene 22.059 observaciones.

\begin{table}[H]
	\centering
	\caption{Base de datos Economicos.cl}
	\label{data-economicos}
    \begin{tabular}{|l|m{30em}|}
        \hline
        \rowcolor[gray]{0.8}
        Variable & Descripción \\
        \hline
        url & URL de la publicación \\
        \hline
        Descripción & Detalles de la publicación \\
        \hline
        price & Valor de venta, en dólares, UF o pesos \\
        \hline
        property\_type & Clase de propiedad: Casa, Departamento, etc. \\
        \hline
        transaction\_type & Clase de transacción\: Arriendo, Venta \\
        \hline
        state & Región de la publicación \\
        \hline
        county & Comuna de la publicación \\
        \hline
        publication\_date & Fecha de la publicación \\
        \hline
        rooms & Cantidad de dormitorios \\
        \hline
        bathrooms & Cantidad de baños \\
        \hline
        m\_built & Extensión del área habitable en metros cuadrados \\
        \hline
        m\_size & Extensión del terreno en metros cuadrados \\
        \hline
        source & Medio de la publicación \\
        \hline
        title & Título de la publicación \\
        \hline
        address & Dirección de la publicación \\
        \hline
        owner & Publicador \\
        \hline
        \_price & Valor convertido a UF del día de la publicación \\
        \hline
    \end{tabular}
\end{table}




\subsection{Computación y Software}
\label{subsec:computacion}
Para efectuar los experimentos, se recurrió a un equipo informático con las especificaciones técnicas detalladas en la Tabla \ref{tabla-componentes-pc}. El procesador seleccionado fue un AMD Ryzen 9 7950X 16-Core Procesadores, complementado con cuatro módulos de 32 GB para sumar una memoria total de 128 GB DDR5. La tarjeta gráfica incorporada fue una NVIDIA GeForce RTX 4090, y el equipo contó con dos discos duros de 500 GB SSD. El uso de un sistema con estas características garantizó una ejecución eficaz de los modelos de generación de datos, asegurando la viabilidad de los experimentos. Cabe resaltar que la selección de los componentes del equipo se realizó de manera meticulosa para garantizar que los resultados obtenidos no se vieran afectados por una capacidad de hardware limitada.

En lo que respecta al software, se empleó el sistema operativo Ubuntu 20.04.2 LTS y se utilizó el lenguaje de programación Python 3.10 para la implementación de los modelos de generación de datos. Se recurrió a diversas bibliotecas, incluyendo DVC, SDV y PyTorch, cuya lista completa está disponible en el \href{https://github.com/gvillarroel/synthetic-data-for-text/blob/main/freeze.txt}{repositorio en Github}. La elección de estas herramientas estuvo guiada por su compatibilidad con el modelo Tddpm, el cual fue empleado en algunos de los experimentos.


\begin{table}[H]
	\centering
	\caption{Computador Usado}
	\label{tabla-componentes-pc}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor[gray]{0.8}
        Componente & Descripción \\
        \hline
        Procesador & AMD Ryzen 9 7950X 16-Core Processor \\
        \hline
        Memoria RAM & 128 GB DDR5 \\
        \hline
        Tarjeta gráfica & NVIDIA GeForce RTX 4090 \\
        \hline
        Disco duro & 1 TB SSD \\
        \hline
      \end{tabular}        
\end{table}  

Con el objetivo de asegurar la reproducibilidad, se implementó \textit{devcontainer}, que configura el entorno de desarrollo y pruebas mediante una imagen replicable de \textit{Docker}. Los experimentos pueden ser reproducidos utilizando el contenedor descrito en el repositorio y el Código \ref{devcontainer-file}.

\begin{listing}[H]
\inputminted[
    framesep=5pt, rulecolor=gray,
    fontsize=\small,
    linenos=true, 
    breaklines=true,xleftmargin=1.0cm
    ]{json}{../../.devcontainer/devcontainer.json}
\caption{Devcontainer del proyecto en curso.}
\label{devcontainer-file}
\end{listing}


El código fuente de los modelos destinados a la generación de datos, así como los scripts para el análisis y la representación gráfica de los resultados, se encuentran disponibles en un repositorio público de Github:
\href{https://github.com/gvillarroel/synthetic-data-for-text}{gvillarroel/synthetic-data-for-text}. Se requiere el uso de DVC para la descarga de datos desde un directorio compartido en Google Drive.

\newpage

\section{Desarrollo del flujo de procesamiento}
\label{subsec:procesamiento}
En las siguientes secciones se detalla el flujo de procesamiento implementado para la generación de nuevos datos sintéticos. Este flujo se inspira en el propuesto por Synthetic Data Vault (SDV), incorporando algunas modificaciones para preservar etapas intermedias.

SDV es un ecosistema de bibliotecas para la generación de datos sintéticos que facilita a los usuarios aprender de bases de datos unidimensionales, multidimensionales y de series temporales, para posteriormente generar nuevos datos sintéticos que mantengan las mismas propiedades estadísticas y el mismo formato que las bases de datos originales. Para conseguir esto, SDV emplea diversas técnicas, como modelos generativos y redes neuronales, con el fin de aprender la distribución subyacente de los datos y generar nuevos datos que sigan dicha distribución \cite{kotelnikov_overview_nodate, patki_synthetic_2016}.

A continuación, se explica el proceso de generación de datos sintéticos para una base de datos unidimensional utilizando la biblioteca Synthetic Data Vault (SDV), seguido de las modificaciones introducidas para expandir el proceso e incorporar nuevos modelos.

En la Tabla \ref{process-sdv} se muestran los pasos necesarios para generar un conjunto de datos sintéticos utilizando SDV:

\begin{figure}[H]
	\centering
	\includesvg[scale=.2,inkscapelatex=false]{../dfd/images/processor.svg}
	\caption{Proceso para generar datos sintéticos con SDV}
	\label{process-sdv}
\end{figure}

\begin{enumerate}
    \item \textbf{Creación de Metadatos}: Se elabora un diccionario que define los campos del conjunto de datos y los tipos de datos que contiene. Esto le permite a SDV aprender la estructura del conjunto de datos original y utilizarla para generar nuevos datos sintéticos con la misma estructura.
    \item \textbf{Creación del Modelo}: Se selecciona el modelo de generación de datos a utilizar. SDV proporciona varios modelos, entre ellos GaussianCopula, CTGAN, CopulaGAN y TVAE, que se adaptan a distintos tipos de datos y distribuciones.
    \item \textbf{Entrenamiento del Modelo}: El modelo seleccionado se entrena con el conjunto de datos original para aprender sus distribuciones y patrones estadísticos.
    \item \textbf{Generación del Conjunto de Datos Sintéticos}: Con el modelo ya entrenado, se generan nuevos datos sintéticos que mantienen la misma estructura y características estadísticas que el conjunto original. Este nuevo conjunto de datos puede ser empleado para diversas aplicaciones, como pruebas de software o análisis de datos sensibles.
\end{enumerate}
Es crucial señalar que el proceso de generación de datos sintéticos con SDV es escalable y puede aplicarse a bases de datos unidimensionales, multidimensionales y de series temporales. Adicionalmente, en este proyecto se introdujeron ciertas modificaciones al flujo para expandir el proceso y facilitar la incorporación de nuevos modelos.
\newpage
En el proceso extendido de generación de datos sintéticos con SDV, se introducen dos nuevas etapas para permitir el almacenamiento de los modelos intermedios y los resultados de la evaluación. El proceso completo se ilustra en la Figura \ref{process-sdv-2} y comprende los siguientes pasos:


\begin{enumerate}
    \item \textbf{Creación de Metadatos}: Se elabora un diccionario que define los campos del conjunto de datos y los tipos de datos que contiene.
    \item \textbf{Creación del Modelo}: Se selecciona el modelo a utilizar. SDV permite elegir entre GaussianCopula, CTGAN, CopulaGAN y TVAE.
    \item \textbf{Entrenamiento del Modelo}: El modelo seleccionado se entrena con el conjunto de datos original para aprender sus distribuciones.
    \item \textbf{Guardado del Modelo}: El modelo entrenado se almacena en un archivo para su uso posterior.
    \item \textbf{Generación del Conjunto de Datos Sintéticos}: Se genera un nuevo conjunto de datos utilizando el modelo entrenado.
    \item \textbf{Evaluación y Guardado de Métricas}: Se evalúa el conjunto de datos sintético generado y se almacenan las métricas, como la correlación, el error absoluto medio y el error cuadrático medio.
\end{enumerate}


\begin{figure}[H]
	\centering
	\includesvg[scale=.15,inkscapelatex=false]{../dfd/images/processor_edited.svg}
	\caption{Proceso para generar datos sintéticos completo}
	\label{process-sdv-2}
\end{figure}

Con estas nuevas etapas, se pueden guardar los modelos intermedios y los resultados de la evaluación, lo que permite una mayor flexibilidad en el proceso y la capacidad de utilizar los modelos y los resultados en posteriores experimentos.



\section{Modelos de generación de datos}
\label{subsec:generacion}

Los modelos de generación de datos tabulares se fundamentan en la metodología propuesta por \emph{Synthetic Data Vault} (SDV), mientras que los modelos de generación de texto utilizan la biblioteca Hugging Face para cargar, realizar \emph{fine-tuning} con nuevas tareas y evaluar el modelo basado en mT5.

\subsection{Modelos para datos tabulares}
Para que un modelo sea compatible con SDV, debe implementar los siguientes métodos:
\begin{enumerate}
    \item \textbf{cargar} (load): Carga el modelo desde un archivo.
    \item \textbf{entrenar} (fit): Entrena el modelo, tomando como entrada un dataframe de pandas.
    \item \textbf{guardar} (save): Almacena el modelo en un archivo.
    \item \textbf{muestrear} (sample): Genera un conjunto de nuevos registros utilizando el modelo entrenado.
\end{enumerate}


Como consideración adicional, se aconseja llevar a cabo el proceso mediante un script en lugar de un cuaderno Jupyter, dado que se ha observado que el cuaderno puede encontrar problemas con algunos modelos debido a restricciones de memoria. A continuación, se especifican los pasos para la ejecución del proceso:
\begin{enumerate}
\item Generar un archivo de configuración que contenga la información requerida para la generación de datos sintéticos, como la ruta del conjunto de datos original y la configuración de los modelos a emplear.
\item Crear un script que cargue la configuración, ejecute el proceso de generación de datos sintéticos y almacene el conjunto de datos sintético resultante.
\item Poner en marcha el script creado en el paso previo.
\end{enumerate}


De esta forma, es posible automatizar el proceso de generación de datos sintéticos y aprovechar una mayor capacidad de procesamiento, lo que puede mejorar el rendimiento del proceso y reducir los tiempos de ejecución.

La clase \emph{Synthetic} es una implementación que permite configurar los modelos a utilizar en el proceso de generación de datos sintéticos. Esta clase encapsula los métodos comunes de los modelos, como \emph{load}, \emph{fit}, \emph{save} y \emph{sample}, lo que permite una configuración general de las entradas y la selección de modelos.


En el ejemplo presentado en el Código \ref{code-economicos-synthetic}, se crea una instancia de la clase \emph{Synthetic} utilizando un pandas dataframe previamente pre procesado. Se especifican las columnas que se considerarán como categorías, las que se considerarán como texto y las que se excluirán del análisis. Además, se indica el directorio donde se almacenarán los archivos temporales, se seleccionan los modelos a utilizar, se establece el número de registros sintéticos deseados y se define una columna objetiva para realizar pruebas con aprendizaje automático y estratificar los conjuntos parciales de datos que se utilizarán. De esta manera, se configura de forma flexible el proceso de generación de datos sintéticos según las necesidades específicas del usuario.

\begin{listing}[H]
\inputminted[
    firstline=45, lastline=54
    ]{python}{../../notebooks/economicos_train.py}
\caption{Instanciando clase Synthetic}
\label{code-economicos-synthetic}
\end{listing}

La Tabla \ref{synthetic-input} presenta las opciones para la instancia de la clase \emph{Synthetic}:

\begin{table}[H]
\centering
\caption{Variables de entrada para \emph{Synthetic}}
\label{synthetic-input}
\begin{tabular}{|l|m{25em}|}
\hline
\rowcolor[gray]{0.8}
\textbf{Variable} & \textbf{Descripción} \\
\hline
df & Pandas DataFrame a utilizar \\
\hline
Id & Nombre de la columna a ser usada como identificadora \\
\hline
category\_columns & Listado de columnas categóricas \\
\hline
text\_columns & Listado de columnas de texto \\
\hline
exclude\_columns & Listado de columnas que deben ser excluidas \\
\hline
synthetic\_folder & Carpeta donde se guardarán los documentos intermedios y finales \\
\hline
models & Listado de modelos a utilizar \\
\hline
n\_sample & Número de registros a generar \\
\hline
target\_column & Columna a utilizar como objetivo para modelos de \emph{machine learning} en las evaluaciones y separación cuando se deba estratificar los campos. \\
\hline
\end{tabular}
\end{table}

En la Tabla \ref{modelos-tab-soportados} se detallan los modelos actualmente soportados en la clase \emph{Synthetic} y su origen.

\begin{table}[H]
	\centering
	\caption{Modelos Tabulares Soportados}
	\label{modelos-tab-soportados}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor[gray]{0.8}
        Nombre Modelo & Fuente \\
        \hline
        copulagan & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        tvae & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        gaussiancopula & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        ctgan & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        tablepreset & SDV \cite{kotelnikov_overview_nodate} \\
        \hline
        smote-enc & tabDDPM \cite{akim_tabddpm_2023} \\
        \hline
        tddpm\_mlp & tabDDPM \cite{akim_tabddpm_2023} \\
        \hline
      \end{tabular}        
\end{table} 

Al ejecutar el script de generación de datos sintéticos, se crearán múltiples archivos en una carpeta. En la Tabla \ref{synth-folders} se muestra un ejemplo de los archivos generados y su formato. El nombre del modelo utilizado se indica en el campo \textbf{<model>}, y en caso de haberse aplicado \emph{Differential Privacy} para generar una versión con ruido. El campo \textbf{<n\_sample>} indica el número de registros sintéticos generados, y finalmente el campo \textbf{<type\_comparison>} especifica si se trata de una comparación entre los datos sintéticos y los datos de entrenamiento (\emph{Synthetic vs Train}, abreviado como ST) o entre los datos sintéticos y los datos de validación (\emph{Synthetic vs Hold}, abreviado como SH). Adicionalmente se encuentran los archivos de esquema (\emph{metadata.json}) y una separación del dataset inicial en el conjunto de entrenamiento y test (hold).
\begin{figure}[H]
	\centering
	\includesvg[scale=1,inkscapelatex=false]{../dfd/images/synth_tree.svg}
	\caption{Carpetas y archivos esperados generados por \emph{Synthetic}}
	\label{synth-folders}
\end{figure}


\newpage
\subsection{Modelos para textos}
Como se mencionó anteriormente, se utilizó el modelo \textbf{mT5} que se entrenó para una nueva tarea utilizando la estrategia presentada en el artículo \emph{Text-to-Text Pre-Training for Data-to-Text Tasks} \cite{kale_text--text_2020}. Para ilustrar el proceso, se presenta un ejemplo del texto pre-procesado, el segmento de la pregunta y la respuesta esperada para un registro del conjunto de datos \emph{economicos.cl}.

\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Input]
<fecha, 2022-01-01>\\
<precio, \$ 105.000.000>\\
<tipo, Departamento>\\
<transacci\'on, Venta>\\
<regi\'on, Metropolitana de Santiago>\\
<comuna, Santiago>\\
<dormitorios, 3.0>\\
<ba\~nos, 3.0>\\
<construidos, 47.0>\\
<terreno, 47.0>\\
<precio\_real, 3387.4540447373292>\\
<titulo, Departamento en Venta en Santiago 3 dormitorios 1 ba\~no>\\
<direcci\'on, DEPARTAMENTO EN EL CORAZON DE LO BARNECHEA Santiago, Metropolitana de Santiago>
\end{tcolorbox}
\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Tarea]
descripci\'on de esta publicaci\'on
\end{tcolorbox}
\begin{tcolorbox}[colback=white,colframe=black!50!white,title=Respuesta esperada]
\footnotesize
Kazona Propiedades Vende Departamento de 47m2, 3 dormitorios, 1 ba\~no, cocina, living comedor , Paredes con Cer\'amica y Tabiquer\'ia en techo con madera barnizada timbrada, ventanas nuevas de PVC y vidrio termolaminado, sistema el\'ectrico actualizado, departamento ubicado en el 3er nivel (sin ascensor) , bajo gasto com\'un. Excelentes conectividades y ubicaci\'on en Pleno Centro De Lo Barnechea, como colegios privados y p\'ublicos, supermercados, Mall Portal La Dehesa, locomoci\'on, entre otros.\\
Podemos destacar de la propiedad:\\
Pleno Centro Lo Barnechea\\
100 metros de locomoci\'on a Escuela Militar , Bilbao, Stgo Centro, Mapocho\\
200 metros colegios Montessori Nido de \'Aguila, San Rafael , otros\\
200 metros Mall Portal La Dehesa\\
200 metros Sta. Isabel\\
300 metros carabineros\\
Gastos comunes bajos  \$10.000\\
Estacionamiento comunitario\\
No paga contribuciones\\
Contactanos al telefono Kazona 569 56031154
\end{tcolorbox}

\section{Obtención de Métricas}
\label{subsec:metricas}

Se han automatizado la mayoría de las métricas para evaluar los conjuntos de datos sintéticos mediante el módulo \emph{metrics}. Estas métricas se aplican a los tres conjuntos de datos para su evaluación, lo que permite calcular estadísticas y comparativas para el conjunto de datos real utilizado para el entrenamiento (train dataset), el conjunto de datos reservado para la evaluación (hold) y el conjunto de datos sintético generado por los diferentes modelos (synthetic). Se pueden recolectar ejecutando el ejemplo de código proporcionado en Código \ref{code-economicos-metrics}.

\begin{listing}[H]
\inputminted[
    firstline=55, lastline=55
    ]{python}{../../notebooks/economicos_run-a.py}
\caption{Obtención de métricas en economicos\_run-a.py}
\label{code-economicos-metrics}
\end{listing}

En la Tabla \ref{metricas-numericas} se muestra las metricas recolectadas para campos numericos.

\begin{longtable}{|m{10em}|m{25em}|}
    \caption{Metricas para campos numericos} 
    \label{metricas-numericas} \\
    \hline
    \rowcolor[gray]{0.8}
    Campo & Ejemplos \\
    \hline
    \endfirsthead
    
    \hline
    \rowcolor[gray]{0.8}
    Campo & Ejemplos \\
    \hline
    \endhead
    
    \hline \multicolumn{2}{|r|}{{Continúa en la siguiente página}} \\ \hline
    \endfoot
    
    \hline \hline
    \endlastfoot
    
    \hline
    Nombre del campo (name) & sqft\_living \\ \hline
    Valores del Top 5 (top5) & [1400 1300 1720 1250 1540] \\ \hline
    Frecuencia Top 5 (top5\_frec) & [109 107 106 106 105] \\ \hline
    Probabilidades de Top 5 (top5\_prob) & [0.00630422 0.00618855 0.00613071 0.00613071 0.00607287] \\ \hline
    Elementos observados (nobs) & 17290 \\ \hline
    Nulos (missing) & 0 \\ \hline
    Promedio (mean) & 2073.894910 \\ \hline
    Desviación Estándar (std) & 907.297963 \\ \hline
    Error estándar de la media (std\_err) & 6.900053 \\ \hline
    Intervalo de confianza superior (upper\_ci) & 2087.418766 \\ \hline
    Intervalo de confianza inferior (lower\_ci) & 2060.371055 \\ \hline
    Rango intercuartílico (iqr) & 1110 \\ \hline
    Rango intercuartílico normalizado (iqr\_normal) & 822.844231 \\ \hline
    Desviación absoluta de la mediana (mad) & 693.180169 \\ \hline
    Desviación absoluta de la mediana normalizada (mad\_normal) & 868.772506 \\ \hline
    Coeficiente de variación (coef\_var) & 0.437485 \\ \hline
    Rango (range) & 11760 \\ \hline
    Valor máximo (max) & 12050 \\ \hline
    Valor mínimo (min) & 290 \\ \hline
    Sesgo (skew) & 1.370859 \\ \hline
    Curtosis (kurtosis) & 7.166622 \\ \hline
    Test de normalidad de Jarque-Bera (jarque\_bera) & 17922.347382 \\ \hline
    Valor p del test de normalidad de Jarque-Bera (jarque\_bera\_pval) & 0 \\ \hline
    Moda (mode) & 1400 \\ \hline
    Frecuencia de la moda (mode\_freq) & 0.006304 \\ \hline
    Mediana (median) & 1910 \\ \hline
    Percentil 0.1\% & 522.890000 \\ \hline
    Percentil 1\% & 720 \\ \hline
    Percentil 5\% & 940 \\ \hline
    Percentil 25\% & 1430 \\ \hline
    Percentil 75\% & 2540 \\ \hline
    Percentil 95\% & 3740 \\ \hline
    Percentil 99\% & 4921.100000 \\ \hline
    Percentil 99.9\% & 6965.550000 \\ \hline
\end{longtable}

En la Tabla \ref{metricas-categoricas} se muestran los datos calculados para campos categóricos.

\begin{table}[H]
    \centering
    \caption{Métricas para campos categóricos}
    \label{metricas-categoricas}
    \begin{tabular}{|m{10em}|m{25em}|}
        \hline
        \rowcolor[gray]{0.8}
        Nombre del campo (name) & waterfront \\ \hline
        Valores del Top 5 (top5) & [0 1] \\ \hline
        Frecuencia Top 5 (top5\_freq) & [17166   124] \\ \hline
        Probabilidades de Top 5 (top5\_prob) & [0.99282822 0.00717178] \\ \hline
        Elementos observados (nobs) & 17290.0 \\ \hline
        Nulos (missing) & 17290.0 \\ \hline
    \end{tabular}
\end{table}

En el Código \ref{codigo-show-score}, se muestra cómo se calcula y se muestra el puntaje promedio para una selección específica de modelos. El código utiliza la función "sort\_values" para ordenar los resultados en orden descendente según el puntaje. Luego, se filtran los resultados para incluir solo los modelos seleccionados y las columnas que muestran el puntaje y la Distancia al registro más cercano (DCR) en los tres umbrales \emph{Synthetic vs Train} (ST), \emph{Synthetic vs Hold} (SH) y \emph{Train vs Hold} TH.
\begin{listing}[H]
    \begin{minted}[linenos=true,frame=lines,framesep=2mm,baselinestretch=1.2]{python}
avg = syn.scores[syn.scores["type"] == "avg"]
avg.sort_values("score", ascending=False).loc[ ["tddpm_mlp","smote-enc","gaussiancopula","tvae","gaussiancopula", "copulagan","ctgan"], ["score", "DCR ST 5th", "DCR SH 5th", "DCR TH 5th"]]
    \end{minted}
\caption{Mostrando Puntajes Promedios Calculados}
\label{codigo-show-score}
\end{listing}
