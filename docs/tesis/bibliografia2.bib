
@software{gvillarroel_data_2022,
	title = {Data Sintética Privada, Generación Vía Modelo Deep Learning},
	url = {https://github.com/gvillarroel/synthetic-data-for-text},
	author = {gvillarroel},
	urldate = {2023-07-03},
	date = {2022-10-18},
	note = {original-date: 2022-04-20T23:08:36Z},
}

@software{noauthor_virtual_2022,
	title = {Virtual Data Lab ({VDL})},
	rights = {{GPL}-3.0},
	url = {https://github.com/mostly-ai/virtualdatalab},
	abstract = {Benchmarking synthetic data generators for sequential data in terms of accuracy and privacy.},
	publisher = {{MOSTLY} {AI}},
	urldate = {2023-07-02},
	date = {2022-11-14},
	note = {original-date: 2020-07-08T16:46:38Z},
	keywords = {privacy, sequential-data, synthetic-data},
}

@online{noauthor_synthetic_nodate,
	title = {Synthetic Data Metrics},
	url = {https://docs.sdv.dev/sdmetrics/},
	urldate = {2023-06-17},
	langid = {english},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/89K2X3WF/sdmetrics.html:text/html},
}

@online{noauthor_open_nodate,
	title = {Open {LLM} Leaderboard - a Hugging Face Space by {HuggingFaceH}4},
	url = {https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	abstract = {Discover amazing {ML} apps made by the community},
	urldate = {2023-06-24},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/M4UKV355/open_llm_leaderboard.html:text/html},
}

@online{noauthor_falcon_nodate,
	title = {Falcon {LLM}},
	url = {https://falconllm.tii.ae/},
	abstract = {Falcon {LLM}, a foundational large language model ({LLM}) with 40 billion parameters},
	urldate = {2023-06-21},
	langid = {english},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/6ACEFZ33/falconllm.tii.ae.html:text/html},
}

@misc{hoffmann_training_2022,
	title = {Training Compute-Optimal Large Language Models},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), {GPT}-3 (175B), Jurassic-1 (178B), and Megatron-Turing {NLG} (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the {MMLU} benchmark, greater than a 7\% improvement over Gopher.},
	number = {{arXiv}:2203.15556},
	publisher = {{arXiv}},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	urldate = {2023-06-21},
	date = {2022-03-29},
	eprinttype = {arxiv},
	eprint = {2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/gvillarroel/Zotero/storage/6NFECU9G/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/gvillarroel/Zotero/storage/N92JDD82/2203.html:text/html},
}

@online{noauthor_llama_nodate,
	title = {{LLaMA}: Open and Efficient Foundation Language Models - Meta Research},
	url = {https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to...},
	titleaddon = {Meta Research},
	urldate = {2023-06-21},
	langid = {spanish},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/T5LBXCS2/llama-open-and-efficient-foundation-language-models.html:text/html},
}

@misc{anil_palm_2023,
	title = {{PaLM} 2 Technical Report},
	url = {http://arxiv.org/abs/2305.10403},
	doi = {10.48550/arXiv.2305.10403},
	abstract = {We introduce {PaLM} 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor {PaLM}. {PaLM} 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that {PaLM} 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to {PaLM}. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. {PaLM} 2 demonstrates robust reasoning capabilities exemplified by large improvements over {PaLM} on {BIG}-Bench and other reasoning tasks. {PaLM} 2 exhibits stable performance on a suite of responsible {AI} evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, {PaLM} 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the {PaLM} 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
	number = {{arXiv}:2305.10403},
	publisher = {{arXiv}},
	author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, {YaGuang} and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
	urldate = {2023-06-21},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.10403 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/gvillarroel/Zotero/storage/AZRWWQUG/Anil et al. - 2023 - PaLM 2 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/home/gvillarroel/Zotero/storage/YDGVZ9HH/2305.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI}},
	urldate = {2023-06-21},
	date = {2023-03-27},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/gvillarroel/Zotero/storage/N7UH45AM/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/home/gvillarroel/Zotero/storage/PW59VXW8/2303.html:text/html},
}

@article{zhao_ctab-gan_2022,
	title = {{CTAB}-{GAN}+: Enhancing Tabular Data Synthesis},
	url = {https://arxiv.org/abs/2204.00401},
	journaltitle = {{arXiv} preprint {arXiv}:2204.00401},
	author = {Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y},
	date = {2022},
}

@online{kotelnikov_overview_nodate,
	title = {Overview — {SDV} 0.18.0 documentation},
	url = {https://sdv.dev/SDV/},
	author = {Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
	urldate = {2023-02-26},
	file = {Overview — SDV 0.18.0 documentation:/home/gvillarroel/Zotero/storage/GT9424L2/SDV.html:text/html},
}

@software{akim_tabddpm_2023,
	title = {{TabDDPM}: Modelling Tabular Data with Diffusion Models},
	url = {https://github.com/rotot0/tab-ddpm},
	shorttitle = {{TabDDPM}},
	author = {Akim},
	urldate = {2023-03-01},
	date = {2023-03-01},
	note = {original-date: 2022-10-02T23:01:07Z},
	keywords = {deep-learning, diffusion-models, pytorch, tabular},
}

@online{kaggle_house_2015,
	title = {House Sales in King County, {USA}},
	url = {https://www.kaggle.com/datasets/harlfoxem/housesalesprediction},
	abstract = {Predict house price using regression},
	author = {Kaggle, {HARLFOXEM}},
	date = {2015},
}

@article{andrejczuk_table--text_2022,
	title = {Table-To-Text generation and pre-training with {TabT}5},
	journaltitle = {{arXiv} preprint {arXiv}:2210.09162},
	author = {Andrejczuk, Ewa and Eisenschlos, Julian Martin and Piccinno, Francesco and Krichene, Syrine and Altun, Yasemin},
	date = {2022},
}

@article{herzig_tapas_2020,
	title = {{TaPas}: Weakly supervised table parsing via pre-training},
	journaltitle = {{arXiv} preprint {arXiv}:2004.02349},
	author = {Herzig, Jonathan and Nowak, Pawe\{{\textbackslash}textbackslash\}l Krzysztof and Müller, Thomas and Piccinno, Francesco and Eisenschlos, Julian Martin},
	date = {2020},
}

@article{devlin_bert_2018,
	title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
	journaltitle = {{arXiv} preprint {arXiv}:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	date = {2018},
}

@article{kale_text--text_2020,
	title = {Text-to-text pre-training for data-to-text tasks},
	journaltitle = {{arXiv} preprint {arXiv}:2005.10433},
	author = {Kale, Mihir and Rastogi, Abhinav},
	date = {2020},
}

@article{xu_modeling_2019,
	title = {Modeling tabular data using conditional gan},
	volume = {32},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
	date = {2019},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: synthetic minority over-sampling technique},
	volume = {16},
	pages = {321--357},
	journaltitle = {Journal of artificial intelligence research},
	author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
	date = {2002},
}

@inproceedings{patki_synthetic_2016,
	title = {The synthetic data vault},
	pages = {399--410},
	booktitle = {2016 {IEEE} International Conference on Data Science and Advanced Analytics ({DSAA})},
	publisher = {{IEEE}},
	author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	date = {2016},
}

@article{kotelnikov_tabddpm_2022,
	title = {{TabDDPM}: Modelling Tabular Data with Diffusion Models},
	journaltitle = {{arXiv} preprint {arXiv}:2209.15421},
	author = {Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
	date = {2022},
}

@inproceedings{zhao_ctab-gan_2021,
	title = {{CTAB}-{GAN}: Effective Table Data Synthesizing},
	volume = {157},
	url = {https://proceedings.mlr.press/v157/zhao21a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation ({GDPR})) unfortunately limit its full effectiveness. Synthetic tabular data emerges as an alternative to enable data sharing while fulfilling regulatory and privacy constraints. The state-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks ({GAN}) and address two main data types in industry, i.e., continuous and categorical. In this paper, we develop {CTAB}-{GAN}, a novel conditional table {GAN} architecture that can effectively model diverse data types, including a mix of continuous and categorical variables. Moreover, we address data imbalance and long tail issues, i.e., certain variables have drastic frequency differences across large values. To achieve those aims, we first introduce the information loss, classification loss and generator loss to the conditional {GAN}. Secondly, we design a novel conditional vector, which efficiently encodes the mixed data type and skewed distribution of data variable. We extensively evaluate {CTAB}-{GAN} with the state of the art {GANs} that generate synthetic tables, in terms of data similarity and analysis utility. The results on five datasets show that the synthetic data of {CTAB}-{GAN} remarkably resembles the real data for all three types of variables and results into higher accuracy for five machine learning algorithms, by up to 17\%.},
	pages = {97--112},
	booktitle = {Proceedings of The 13th Asian Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y.},
	editor = {Balasubramanian, Vineeth N. and Tsang, Ivor},
	date = {2021-11-17},
}

@article{acharya_gensyn_2022,
	title = {{GenSyn}: A Multi-stage Framework for Generating Synthetic Microdata using Macro Data Sources},
	journaltitle = {{arXiv} preprint {arXiv}:2212.05975},
	author = {Acharya, Angeela and Sikdar, Siddhartha and Das, Sanmay and Rangwala, Huzefa},
	date = {2022},
}

@article{pujol_prefair_2022,
	title = {{PreFair}: Privately Generating Justifiably Fair Synthetic Data},
	journaltitle = {{arXiv} preprint {arXiv}:2212.10310},
	author = {Pujol, David and Gilad, Amir and Machanavajjhala, Ashwin},
	date = {2022},
}

@article{borisov_language_2022,
	title = {Language models are realistic tabular data generators},
	url = {https://arxiv.org/pdf/2210.06280.pdf},
	journaltitle = {{arXiv} preprint {arXiv}:2210.06280},
	author = {Borisov, Vadim and Seßler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
	date = {2022},
}

@article{park_data_2018,
	title = {Data synthesis based on generative adversarial networks},
	journaltitle = {{arXiv} preprint {arXiv}:1806.03384},
	author = {Park, Noseong and Mohammadi, Mahmoud and Gorde, Kshitij and Jajodia, Sushil and Park, Hongkyu and Kim, Youngmin},
	date = {2018},
}

@article{thambawita_singan-seg_2022,
	title = {{SinGAN}-Seg: Synthetic training data generation for medical image segmentation},
	volume = {17},
	pages = {e0267976},
	number = {5},
	journaltitle = {{PloS} one},
	author = {Thambawita, Vajira and Salehi, Pegah and Sheshkal, Sajad Amouei and Hicks, Steven A and Hammer, Hugo L and Parasa, Sravanthi and Lange, Thomas de and Halvorsen, Pål and Riegler, Michael A},
	date = {2022},
	note = {Publisher: Public Library of Science San Francisco, {CA} {USA}},
}

@article{rajabi_tabfairgan_2022,
	title = {Tabfairgan: Fair tabular data generation with generative adversarial networks},
	volume = {4},
	pages = {488--501},
	number = {2},
	journaltitle = {Machine Learning and Knowledge Extraction},
	author = {Rajabi, Amirarsalan and Garibay, Ozlem Ozmen},
	date = {2022},
	note = {Publisher: {MDPI}},
}

@article{solatorio_realtabformer_2023,
	title = {{REaLTabFormer}: Generating Realistic Relational and Tabular Data using Transformers},
	journaltitle = {{arXiv} preprint {arXiv}:2302.02041},
	author = {Solatorio, Aivin V and Dupriez, Olivier},
	date = {2023},
}

@inproceedings{dwork_calibrating_2006,
	title = {Calibrating noise to sensitivity in private data analysis},
	pages = {265--284},
	booktitle = {Theory of Cryptography: Third Theory of Cryptography Conference, {TCC} 2006, New York, {NY}, {USA}, March 4-7, 2006. Proceedings 3},
	publisher = {Springer},
	author = {Dwork, Cynthia and {McSherry}, Frank and Nissim, Kobbi and Smith, Adam},
	date = {2006},
}

@inproceedings{dwork_differential_2006,
	title = {Differential privacy},
	pages = {1--12},
	booktitle = {Automata, Languages and Programming: 33rd International Colloquium, {ICALP} 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part {II} 33},
	publisher = {Springer},
	author = {Dwork, Cynthia},
	date = {2006},
}

@article{rosenblatt_differentially_2020,
	title = {Differentially private synthetic data: Applied evaluations and enhancements},
	url = {https://arxiv.org/abs/2011.05537},
	journaltitle = {{arXiv} preprint {arXiv}:2011.05537},
	author = {Rosenblatt, Lucas and Liu, Xiaoyan and Pouyanfar, Samira and de Leon, Eduardo and Desai, Anuj and Allen, Joshua},
	date = {2020},
}

@article{act_health_1996,
	title = {Health insurance portability and accountability act of 1996},
	volume = {104},
	url = {http://www.eolusinc.com/pdf/hipaa.pdf},
	pages = {191},
	journaltitle = {Public law},
	author = {Act, Accountability},
	date = {1996},
}

@article{pardau_california_2018,
	title = {The California consumer privacy act: Towards a European-style privacy regime in the United States},
	volume = {23},
	pages = {68},
	journaltitle = {J. Tech. L. \& Pol'y},
	author = {Pardau, Stuart L},
	date = {2018},
	note = {Publisher: {HeinOnline}},
}

@article{regulation_regulation_2016,
	title = {Regulation ({EU}) 2016/679 of the European Parliament and of the Council},
	volume = {679},
	url = {https://dvbi.ru/Portals/0/DOCUMENTS_SHARE/RISK_MANAGEMENT/EBA/GDPR_eng_rus.pdf},
	pages = {2016},
	journaltitle = {Regulation (eu)},
	author = {Regulation, Protection},
	date = {2016},
}

@article{de_capitani_di_vimercati_data_2012,
	title = {Data privacy: Definitions and techniques},
	volume = {20},
	pages = {793--817},
	number = {6},
	journaltitle = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {De Capitani Di Vimercati, Sabrina and Foresti, Sara and Livraga, Giovanni and Samarati, Pierangela},
	date = {2012},
	note = {Publisher: World Scientific},
}

@article{adnan_analytical_2019,
	title = {An analytical study of information extraction from unstructured and multidimensional big data},
	volume = {6},
	url = {https://link.springer.com/article/10.1186/s40537-019-0254-8},
	pages = {1--38},
	journaltitle = {Journal of Big Data},
	author = {Adnan, Kiran and Akbar, Rehan},
	date = {2019},
	note = {Publisher: Springer},
}

@article{gantz_digital_2012,
	title = {The digital universe in 2020: Big data, bigger digital shadows, and biggest growth in the far east},
	volume = {2007},
	url = {https://datastorageasean.com/sites/default/files/idc-the-digital-universe-in-2020.pdf},
	pages = {1--16},
	number = {2012},
	journaltitle = {{IDC} {iView}: {IDC} Analyze the future},
	author = {Gantz, John and Reinsel, David},
	date = {2012},
}

@book{bruce_practical_2020,
	title = {Practical statistics for data scientists: 50+ essential concepts using R and Python},
	publisher = {O'Reilly Media},
	author = {Bruce, Peter and Bruce, Andrew and Gedeck, Peter},
	date = {2020},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \{{\textbackslash}textbackslash\}Lukasz and Polosukhin, Illia},
	date = {2017},
}

@book{el_emam_practical_2020,
	title = {Practical synthetic data generation: balancing privacy and the broad availability of data},
	publisher = {O'Reilly Media},
	author = {El Emam, Khaled and Mosquera, Lucy and Hoptroff, Richard},
	date = {2020},
}

@online{noauthor_exclusive_2023,
	title = {Exclusive: The \$2 Per Hour Workers Who Made {ChatGPT} Safer},
	url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
	shorttitle = {Exclusive},
	abstract = {A {TIME} investigation reveals the difficult conditions faced by the workers who made {ChatGPT} possible},
	titleaddon = {Time},
	urldate = {2023-02-19},
	date = {2023-01-18},
	langid = {english},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/7HRC8MFD/openai-chatgpt-kenya-workers.html:text/html},
}

@article{milmo_google_2023,
	title = {Google v Microsoft: who will win the {AI} chatbot race?},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2023/feb/10/google-v-microsoft-who-will-win-the-ai-chatbot-race-bard-chatgpt},
	shorttitle = {Google v Microsoft},
	abstract = {Bard’s misfire on launch cost owner \$160bn but experts believe {ChatGPT} is also prone to errors},
	journaltitle = {The Guardian},
	author = {Milmo, Dan and editor, Dan Milmo Global technology},
	urldate = {2023-02-19},
	date = {2023-02-10},
	langid = {british},
	keywords = {Alphabet, Artificial intelligence ({AI}), Business, {ChatGPT}, Computing, Google, Microsoft, Technology, Technology sector, {US} news, World news},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/93EMKRV8/google-v-microsoft-who-will-win-the-ai-chatbot-race-bard-chatgpt.html:text/html},
}

@online{noauthor_microsoft_2023,
	title = {Microsoft and Google are in a ‘Game of Thrones’ battle over A.I.— but Apple and Amazon still have huge roles to play, according to Wedbush},
	url = {https://finance.yahoo.com/news/microsoft-google-game-thrones-battle-174112314.html},
	abstract = {Tech companies are racing to figure out how to implement A.I. in their products, and it’s not just about search engines.},
	urldate = {2023-02-19},
	date = {2023-02-15},
	langid = {american},
}

@online{noauthor_chatgpt-initiated_2023,
	title = {{ChatGPT}-initiated {AI} war},
	url = {https://www.koreatimes.co.kr/www/opinion/2023/02/202_345310.html},
	abstract = {Global tech giants are rushing to take the lead in artificial intelligence ({AI}), prompted by the sweeping frenzy of {ChatGPT}. The U.S.' {OpenAI} first unveiled the Chat Generative Pre-trained Transformer ({ChatGPT}) in December last year. Such state-of-the-art technology generated a stir as the number of its monthly users surpassed 100 million in two months, nudging desperate Microsoft and Google to come up with their own new {AI} services.},
	titleaddon = {koreatimes},
	urldate = {2023-02-19},
	date = {2023-02-13},
	langid = {english},
	note = {Section: Opinion},
}

@online{noauthor_imagen_nodate,
	title = {Imagen: Text-to-Image Diffusion Models},
	url = {https://imagen.research.google/},
	urldate = {2023-02-19},
	file = {Imagen\: Text-to-Image Diffusion Models:/home/gvillarroel/Zotero/storage/P5KGIZW2/imagen.research.google.html:text/html},
}

@online{noauthor_stable_nodate,
	title = {Stable Diffusion Public Release},
	url = {https://stability.ai/blog/stable-diffusion-public-release},
	abstract = {We are delighted to announce the public release of Stable Diffusion and the launch of {DreamStudio} Lite.},
	titleaddon = {Stability {AI}},
	urldate = {2023-02-19},
	langid = {british},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/ZCN9GW22/stable-diffusion-public-release.html:text/html},
}

@online{noauthor_dalle_nodate,
	title = {{DALL}·E 2},
	url = {https://openai.com/dall-e-2/},
	abstract = {{DALL}·E 2 is a new {AI} system that can create realistic images and art from a description in natural language.},
	titleaddon = {{OpenAI}},
	urldate = {2023-02-19},
	langid = {english},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/N7ICEGKB/dall-e-2.html:text/html},
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - {ImageNet} Benchmark (Image Classification)},
	url = {https://paperswithcode.com/sota/image-classification-on-imagenet},
	abstract = {The current state-of-the-art on {ImageNet} is {CoCa} (finetuned). See a full comparison of 846 papers with code.},
	urldate = {2023-02-19},
	langid = {english},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/78KURA4J/image-classification-on-imagenet.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2023-02-19},
	date = {2012},
	file = {Full Text PDF:/home/gvillarroel/Zotero/storage/V3M7SMI6/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@misc{openai_chatgpt_2023,
	title = {{ChatGPT}: a large language model trained by {OpenAI}},
	url = {https://openai.com/blog/chatgpt-a-large-scale-generative-language-model/},
	author = {{OpenAI}},
	date = {2023},
}

@online{smith-goodson_ibm_nodate,
	title = {{IBM} Demonstrates Groundbreaking Artificial Intelligence Research Using Foundational Models And Generative {AI}},
	url = {https://www.forbes.com/sites/moorinsights/2023/02/13/ibm-demonstrates-groundbreaking-artificial-intelligence-research-using-foundational-models-and-generative-ai/},
	abstract = {Vice President of {AI} \& Quantum Computing, Paul Smith-Goodson, dives in as he sheds light on the life-saving potential of {AI} by examining its practical applications in the creation of new antibiotics and other scientific {AI} tools.},
	titleaddon = {Forbes},
	author = {Smith-Goodson, Paul},
	urldate = {2023-02-19},
	langid = {english},
	note = {Section: Cloud},
	file = {Snapshot:/home/gvillarroel/Zotero/storage/YM8XEWL7/ibm-demonstrates-groundbreaking-artificial-intelligence-research-using-foundational-models-and-.html:text/html},
}

@incollection{jajodia_k-anonymity_2021,
	location = {Berlin, Heidelberg},
	title = {k-Anonymity},
	isbn = {978-3-642-27739-9},
	url = {https://link.springer.com/10.1007/978-3-642-27739-9_754-2},
	pages = {1--5},
	booktitle = {Encyclopedia of Cryptography, Security and Privacy},
	publisher = {Springer Berlin Heidelberg},
	author = {De Capitani Di Vimercati, Sabrina and Samarati, Pierangela},
	editor = {Jajodia, Sushil and Samarati, Pierangela and Yung, Moti},
	urldate = {2023-11-17},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-642-27739-9_754-2},
	file = {De Capitani Di Vimercati y Samarati - 2021 - k-Anonymity.pdf:/home/gvillarroel/Zotero/storage/C2HGMMQX/De Capitani Di Vimercati y Samarati - 2021 - k-Anonymity.pdf:application/pdf},
}

@article{biryukov_data_nodate,
	title = {Data Encryption Standard ({DES})},
	author = {Biryukov, Alex and Cannière, Christophe De and Limpertsberg, Campus},
	langid = {english},
	file = {Biryukov et al. - Data Encryption Standard (DES).pdf:/home/gvillarroel/Zotero/storage/YHCUI9IK/Biryukov et al. - Data Encryption Standard (DES).pdf:application/pdf},
}
